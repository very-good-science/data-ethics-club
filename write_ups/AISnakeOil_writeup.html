
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Data Ethics Club: AI Snake Oil Book Club &#8212; Data Ethics Club  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=56fe5f99" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'write_ups/AISnakeOil_writeup';</script>
    <link rel="canonical" href="dataethicsclub.com/write_ups/AISnakeOil_writeup.html" />
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />



  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this site..."
         aria-label="Search this site..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt=""/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">Data Ethics Club</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../join_in/join_in.html">
    ü§ó Join In
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../reading-list.html">
    üìñ Reading List
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="write-ups.html">
    üñäÔ∏è Write-ups
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../how_to/reuse_dec.html">
    ‚ô∫ Reuse
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about/about.html">
    ‚ùì About
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/very-good-science/data-ethics-club" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.jiscmail.ac.uk/cgi-bin/webadmin?SUBED1=DATAETHICSCLUB&A=1" title="Mailing List" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-regular fa-envelope fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Mailing List</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://doi.org/10.1016/j.patter.2022.100537" title="DEC Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-readme fa-lg" aria-hidden="true"></i>
            <span class="sr-only">DEC Paper</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../join_in/join_in.html">
    ü§ó Join In
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../reading-list.html">
    üìñ Reading List
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="write-ups.html">
    üñäÔ∏è Write-ups
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../how_to/reuse_dec.html">
    ‚ô∫ Reuse
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about/about.html">
    ‚ùì About
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/very-good-science/data-ethics-club" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.jiscmail.ac.uk/cgi-bin/webadmin?SUBED1=DATAETHICSCLUB&A=1" title="Mailing List" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-regular fa-envelope fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Mailing List</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://doi.org/10.1016/j.patter.2022.100537" title="DEC Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-readme fa-lg" aria-hidden="true"></i>
            <span class="sr-only">DEC Paper</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__postcard">


<h2>
  
  
  <i class="fa fa-calendar"></i>
  
  <span>11 June 2025</span>
  
</h2>
<ul>
  <div class="ablog-sidebar-item ablog__postcard2">


<li id="ablog-sidebar-item author ablog__author">
  <span>
    
    <i class="fa-fw fa fa-user"></i>
    
    </span>
  
  
  <a href="write-ups/author/jessica-woodgate.html">Jessica Woodgate</a>
  
  
  
</li>




<li id="ablog-sidebar-item category ablog__category">
  <span>
    
    <i class="fa-fw fa fa-folder-open"></i>
    
    </span>
  
  
  <a href="write-ups/category/bookclub.html">Bookclub</a>
  
  
  
</li>


<li id="ablog-sidebar-item tags ablog__tags">
  <span>
    
    
    <i class="fa-fw fa fa-tags"></i>
    
    
    </span>
  
  
  <a href="write-ups/tag/predictive-ai.html">predictive AI</a>
  
  
  
  
  
  <a href="write-ups/tag/llms.html">LLMs</a>
  
  
  
  
  
  <a href="write-ups/tag/generative-ai.html">generative AI</a>
  
  
  
  
  
  <a href="write-ups/tag/hype.html">hype</a>
  
  
  
</li>


</div>
</ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__recentposts">
<h3>
  <a href="write-ups.html">Recent Posts</a>
</h3>
<ul>
  
  
  <li>
    <a href="2025-04-30_writeup.html">
      30 April - Data Ethics Club: UK announces AI funding for teachers: how this technology could change the profession
    </a>
  </li>
  
  <li>
    <a href="2025-04-16_writeup.html">
      16 April - Data Ethics Club: Understanding and supporting the mental health and professional quality of life of academic mental health researchers: results from a cross-sectional survey
    </a>
  </li>
  
  <li>
    <a href="2025-04-02_writeup.html">
      02 April - Data Ethics Club: The Political Economy of Death in the Age of Information: A Critical Approach to the Digital Afterlife Industry
    </a>
  </li>
  
  <li>
    <a href="2025-03-19_writeup.html">
      19 March - Data Ethics Club: The Most Useful Thing AI Has Ever Done: AlphaFold
    </a>
  </li>
  
  <li>
    <a href="2025-03-05_writeup.html">
      05 March - Data Ethics Club: How It‚Äôs Unfair to Use Personality Tests in Hiring (International Women‚Äôs Day Special)
    </a>
  </li>
  
</ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__category">
<h3>
  <a href="write-ups/category.html">Categories</a>
</h3>
<ul>
  
  
  <li>
    <a href="write-ups/category/blog.html">Blog (7)</a>
  </li>
  
  
  
  <li>
    <a href="write-ups/category/bookclub.html">Bookclub (2)</a>
  </li>
  
  
  
  <li>
    <a href="write-ups/category/write-up.html">Write Up (71)</a>
  </li>
  
  
</ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__tags">
<link rel="stylesheet" href="../_static/ablog/tagcloud.css" type="text/css" />
<h3><a href="write-ups/tag.html">Tags</a></h3>
<ul class="ablog-cloud">
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/agi.html">AGI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/ai.html">AI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/ai-applications.html">AI applications</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/ai-ethics.html">AI ethics</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/ai-in-military.html">AI in military</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/alphafold.html">AlphaFold</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/chatgpt.html">ChatGPT</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/fair.html">FAIR</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/khanacademy.html">Khanacademy</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/llms.html">LLMs</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/ml.html">ML</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/new-years-resolutions.html">New Year's Resolutions</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/transparency.html">Transparency</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/ableism.html">ableism</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/abuse.html">abuse</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/art.html">art</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-3">
    <a href="write-ups/tag/automation.html">automation</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-5">
    <a href="write-ups/tag/bias.html">bias</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/big-tech.html">big tech</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/bots.html">bots</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/bullshit.html">bullshit</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/chatbots.html">chatbots</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/children.html">children</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/collective-action.html">collective action</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/communication.html">communication</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/computer-vision.html">computer vision</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/consent.html">consent</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/consumerism.html">consumerism</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/coproduction.html">coproduction</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-2">
    <a href="write-ups/tag/crime.html">crime</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/data-feminism.html">data feminism</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/data-labelling.html">data labelling</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/data-stewardship.html">data stewardship</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/data-week.html">data week</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/dataethics.html">dataethics</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/decolonial-ai.html">decolonial AI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/decolonisation.html">decolonisation</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/decolonising.html">decolonising</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/deep-fakes.html">deep fakes</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/digital-afterlife.html">digital afterlife</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-2">
    <a href="write-ups/tag/education.html">education</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/enshittification.html">enshittification</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/environment.html">environment</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/ethics.html">ethics</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/explainability.html">explainability</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/facial-recognition.html">facial recognition</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/fairness.html">fairness</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/feminism.html">feminism</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/gender.html">gender</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-2">
    <a href="write-ups/tag/generative-ai.html">generative AI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-3">
    <a href="write-ups/tag/generativeai.html">generativeAI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/genetic-testing.html">genetic testing</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/global-south.html">global south</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/google.html">google</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/government-use-of-ai.html">government use of AI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/group-projects.html">group projects</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/hallucination.html">hallucination</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/healthcare.html">healthcare</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/history.html">history</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/human-like-ai.html">human-like AI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/hype.html">hype</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/images-of-ai.html">images of AI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/injustice.html">injustice</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/interdisciplinary.html">interdisciplinary</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/intersectional-feminism.html">intersectional feminism</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/labour-rights.html">labour rights</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/law.html">law</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/medicine.html">medicine</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/mental-health.html">mental health</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/meta-data-ethics.html">meta data ethics</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/metaphor.html">metaphor</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/myers-briggs.html">myers-briggs</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/nlp.html">nlp</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/open-science.html">open science</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/open-source.html">open source</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/outreach.html">outreach</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/oversight.html">oversight</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/personality-test.html">personality test</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/philosophy.html">philosophy</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/policy.html">policy</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/prediction.html">prediction</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/predictive-ai.html">predictive AI</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-2">
    <a href="write-ups/tag/privacy.html">privacy</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/protein-folding.html">protein folding</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/racism.html">racism</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/recruitment.html">recruitment</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-2">
    <a href="write-ups/tag/research-ethics.html">research ethics</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/resolutions.html">resolutions</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/responsibility.html">responsibility</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/search-engine-optimisation.html">search engine optimisation</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/social.html">social</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/social-media.html">social media</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/sociotechnical-systems.html">sociotechnical systems</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/software-dev.html">software dev</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/standpoint-theory.html">standpoint theory</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/statistics.html">statistics</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/structural-injustice.html">structural injustice</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/synthetic-biology.html">synthetic biology</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/trust.html">trust</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/values.html">values</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/war.html">war</a>
  </li>
  
  
  
  <li class="ablog-cloud ablog-cloud-1">
    <a href="write-ups/tag/workers-rights.html">worker's rights</a>
  </li>
  
  
</ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__authors">
<h3><a href="write-ups/author.html">Authors</a></h3>
<ul>
   
  <li>
    <a href="write-ups/author/dwight-barry.html">Dwight Barry (1)</a>
  </li>
    
  <li>
    <a href="write-ups/author/euan-bennet.html">Euan Bennet (1)</a>
  </li>
    
  <li>
    <a href="write-ups/author/hannah-odonoghue.html">Hannah O‚ÄôDonoghue (1)</a>
  </li>
    
  <li>
    <a href="write-ups/author/huw-day.html">Huw Day (34)</a>
  </li>
    
  <li>
    <a href="write-ups/author/jessica-woodgate.html">Jessica Woodgate (38)</a>
  </li>
    
  <li>
    <a href="write-ups/author/natalie-zelenka.html">Natalie Zelenka (3)</a>
  </li>
    
  <li>
    <a href="write-ups/author/nina-di-cara.html">Nina Di Cara (2)</a>
  </li>
    
  <li>
    <a href="write-ups/author/paul-lee.html">Paul Lee (1)</a>
  </li>
    
  <li>
    <a href="write-ups/author/roman-shkunov.html">Roman Shkunov (1)</a>
  </li>
    
  <li>
    <a href="write-ups/author/vanessa-hanschke.html">Vanessa Hanschke (1)</a>
  </li>
   
</ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__archive">
<h3>
  <a href="write-ups/archive.html">Archives</a>
</h3>
<ul>
  
  
  <li>
    <a href="write-ups/2025.html">2025 (9)</a>
  </li>
  
  
  
  <li>
    <a href="write-ups/2024.html">2024 (20)</a>
  </li>
  
  
  
  <li>
    <a href="write-ups/2023.html">2023 (15)</a>
  </li>
  
  
  
  <li>
    <a href="write-ups/2022.html">2022 (20)</a>
  </li>
  
  
  
  <li>
    <a href="write-ups/2021.html">2021 (16)</a>
  </li>
  
  
</ul>
</div>
</div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Data Ethics Club: AI Snake Oil Book Club</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
<section id="data-ethics-club-ai-snake-oil-book-club">
<h1>Data Ethics Club: <a class="reference external" href="https://www.aisnakeoil.com/">AI Snake Oil</a> Book Club<a class="headerlink" href="#data-ethics-club-ai-snake-oil-book-club" title="Link to this heading">#</a></h1>
<div class="admonition-what-s-this admonition">
<p class="admonition-title">What‚Äôs this?</p>
<p>This is summary of the discussions held in Data Ethics book club summer 2025, where we spoke and wrote about <a class="reference external" href="https://www.aisnakeoil.com/">AI Snake Oil</a> by Arvind Narayanan and Sayash Kapoor.
The summary was written by Jessica Woodgate, who tried to synthesise everyone‚Äôs contributions to this document and the discussion. ‚ÄúWe‚Äù = ‚Äúsomeone at Data Ethics Club‚Äù.
Huw Day helped with the final edit.</p>
</div>
</section>
<section id="chapter-1-introduction">
<h1><a class="reference external" href="https://press.princeton.edu/books/hardcover/9780691249131/ai-snake-oil#preview">Chapter 1 ‚Äì Introduction</a><a class="headerlink" href="#chapter-1-introduction" title="Link to this heading">#</a></h1>
<section id="chapter-summary">
<h2>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Link to this heading">#</a></h2>
<p>The term ‚ÄúAI‚Äù has been appropriated for a multitude of different technologies and applications, confusing discussions about its strengths and limitations, and misleading people about its capabilities. The introduction begins by distinguishing between generative AI, which produces various types of content such as text or images, and predictive AI, which produces outputs forecasting the future to inform decision-making in the present.</p>
<p>The waters surrounding the definition of AI are muddied; influenced by historical usage, marketing, and more. Some problems AI attempts to address, like vacuuming, are solvable; others, like predicting whether someone will commit a crime, are not. Some applications were once thought difficult, but AI has proven to be very good at, such as facial recognition. However, even if the mechanism is good, the tool can fail in practice, e.g., if the data is noisy, biased, or abused for malicious intent.</p>
<p>Sold as an accurate and capable technology, predictive AI has been widely adopted. However, there are significant issues with the premises predictive AI is based on, as it attempts to translate and reduce high dimensional phenomena to computationally feasible outputs. In practice, there is missing data, bias, and participants attempting to game the system. Generative AI, the chapter argues, holds more promise and yet also runs into significant issues. Factual inaccuracies in outputs are common, leading to rampant misuse such as error-filled news articles or AI-generated books.</p>
<p>Misinformation, misunderstandings, and mythology about AI are fed by a combination of researchers, companies, and the media. In research, the buzzier the research topic, the worse the quality of research tends to be. Companies feed off of hype, seeking to maximise profits whilst rarely being transparent about the accuracy of their products. Crumbling revenue in the media combined with access journalism, where outlets rely on good relationships with companies to be able to interview subjects, further propagate hype narratives dictated by AI companies.</p>
<p>The limitations, hype, and misconceptions surrounding AI leads the chapter to analogise AI as a ‚Äúsnake oil‚Äù product: a miracle cure advertised under false pretences. The book aims to identify AI snake oil ‚Äì AI that does not and cannot work.</p>
</section>
<section id="discussion-summary">
<h2>Discussion Summary<a class="headerlink" href="#discussion-summary" title="Link to this heading">#</a></h2>
<section id="what-do-easy-and-hard-mean-in-the-context-of-ai-does-it-refer-to-computational-requirements-or-the-human-effort-needed-to-build-ai-to-perform-a-task-or-something-else-and-what-does-easy-hard-for-people-mean">
<h3>What do ‚Äúeasy‚Äù and ‚Äúhard‚Äù mean in the context of AI? Does it refer to computational requirements, or the human effort needed to build AI to perform a task, or something else? And what does easy/hard for people mean?<a class="headerlink" href="#what-do-easy-and-hard-mean-in-the-context-of-ai-does-it-refer-to-computational-requirements-or-the-human-effort-needed-to-build-ai-to-perform-a-task-or-something-else-and-what-does-easy-hard-for-people-mean" title="Link to this heading">#</a></h3>
<p>The idea of ‚Äúeasy‚Äù or ‚Äúhard‚Äù problems for AI weren‚Äôt specifically defined in the chapter, but it did give some examples of problems previously thought to be difficult for AI which the technology is now very capable of solving, such as image classification. Spellcheck is another example of an application previously thought to be hard but is today so embedded in our everyday lives that we might not even think of it as AI.</p>
<p>Sometimes what is ‚Äúeasy‚Äù or ‚Äúhard‚Äù for AI is contrary to what is easy or hard for humans ‚Äì a phenomenon coined <a class="reference external" href="https://en.m.wikipedia.org/wiki/Moravec%27s_paradox">Moravec‚Äôs paradox</a>. Moravec‚Äôs paradox is the observation that reasoning (which is difficult for humans) requires little computation, but sensory and perception skills (which are easy for humans) are highly computationally expensive. <a class="reference external" href="https://en.m.wikipedia.org/wiki/Computational_complexity">Computational complexity</a> is a well-studied field examining the amount of resources required to run an algorithm. We wondered if easy and hard in relation to AI are somehow related to how difficult it is for AI to produce an accurate output.</p>
<p>Tasks that are easy for humans are those that we can do quickly and without much concentration. Pinpointing what these tasks are is tricky; we don‚Äôt know what we know. People aren‚Äôt good at breaking tasks down into smaller levels than humans are typically used to or deducing which of these tasks are easy. The difficulty with breaking down tasks is why some people find programming really tricky. What counts as easy might depend on the sample population, e.g., a roomful of mechanics will find it much easier to replace an engine than a roomful of mathematicians.</p>
<p>Discerning what is easy or hard for AI is not straightforward, as it depends on the lens you examine the tool through and the context it is situated within. LLMs now seem to be very good at predicting text, giving the impression that the task is easy. However, an extraordinary amount of resources goes into training an LLM, suggesting it is actually a hard problem. Facial recognition is accurate under the right conditions, but there are many drawbacks for how it is used, making it hard to delineate appropriate use cases. Perhaps easy and hard aren‚Äôt the right terms; we should instead be asking how much context is needed to solve the problem, and how hard the execution is.</p>
<p>The underlying message of the introduction is that certain tasks are made easier or harder from the application of certain AI systems, and the way we sell those systems affects how they are used. For example, linear regression is really good at some statistical modelling problems. However, if we sold it as a silver bullet to solve any problem, it would be (and, as the chapter demonstrates, has proven to be) rubbish.</p>
<p>The barriers to entry for AI have been drastically lowered over recent years. On the deployment side, LLMs are now very easy to set up on a personal laptop. On the user side, generative AI interfaces are widely accessible. It is actually starting to require more effort <em>not</em> to use LLMs in some settings such as <a class="reference external" href="https://www.bbc.com/news/articles/cpw77qwd117o">searching</a>, where search engines are displaying an ‚ÄúAI overview‚Äù before presenting the results. Tasks that LLMs previously found difficult are slowly getting easier and easier, making their deployment and user friendliness increasingly accessible. For example, maintaining context over time was something that LLMs previously found very difficult.</p>
<p>However, the <a class="reference external" href="https://www.techtarget.com/whatis/definition/context-window">context window</a>, which dictates how much input the LLM can consider when calculating its output, is slowly getting larger. A larger context window entails that an LLM can consider more information in its answers. In addition, ChatGPT now incorporates some <a class="reference external" href="https://openai.com/index/memory-and-new-controls-for-chatgpt/">‚Äúmemory‚Äù functionality</a>, where information from previous conversations is maintained. However, sometimes relevant context goes beyond conversations, such as body language. This raises important questions about what kind and how much information goes into the model, e.g. whether it should consider current affairs or interpersonal relationships.</p>
<p>More data and computational power could mean that AI gets better, but this is not always true. It may depend on the use case and type of AI. For example, given enough training, AI can beat the best human chess player, but how long will it be before AI can make a cup of tea to your preferences, and is more computational power enough to solve this? If the task is something that the user doesn‚Äôt know much about, or is difficult to express, the usefulness of AI goes down. Training and human feedback is required on both sides. It is difficult to know where the improvements will stop or how the evolution of AI will unfold, especially considering the transformative effect of previous industrial revolutions.</p>
</section>
<section id="based-on-your-definitions-of-these-terms-pick-a-variety-of-tasks-and-try-to-place-them-on-a-2-dimensional-spectrum-where-the-axes-represent-peoples-and-computers-ease-of-performing-the-task-what-sort-of-relationship-do-you-see">
<h3>Based on your definitions of these terms, pick a variety of tasks and try to place them on a 2-dimensional spectrum where the axes represent people‚Äôs and computers‚Äô ease of performing the task. What sort of relationship do you see?<a class="headerlink" href="#based-on-your-definitions-of-these-terms-pick-a-variety-of-tasks-and-try-to-place-them-on-a-2-dimensional-spectrum-where-the-axes-represent-peoples-and-computers-ease-of-performing-the-task-what-sort-of-relationship-do-you-see" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Difficulty</p></th>
<th class="head"><p>Computer</p></th>
<th class="head"><p>Human</p></th>
<th class="head"><p>Both</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>EASY</p></td>
<td><p>Coding</p></td>
<td><p>Sense of right and wrong</p></td>
<td><p>Image classification</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Pattern recognition in big sets of data</p></td>
<td><p>Ironing</p></td>
<td><p>Speech to text</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Analytical decision-making (depending on algorithm and input data)</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Chess</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Remembering lots of information</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Recall (for some computers but difficult for LLMs)</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Writing a poem in a certain style</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Sucking in created content</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>HARD</p></td>
<td><p>Ironing</p></td>
<td><p>Coding</p></td>
<td><p>Creativity</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Ethical decisions / judicial</p></td>
<td><p>Content creation</p></td>
<td><p>Reading emotions</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Holding/understanding context</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Sarcasm/jokes</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Moving around in a new environment</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Certainty of answers</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Language manipulation</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p><a class="reference external" href="https://www.youtube.com/watch?v=FAspMnu4Rt0">Counting ‚Äúr‚Äôs‚Äù in the word ‚Äústrawberry‚Äù</a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="the-text-gives-many-examples-of-ai-that-quietly-work-well-like-spellcheck-can-you-think-of-other-examples-what-do-you-think-are-examples-of-tasks-that-ai-cant-yet-perform-reliably-but-one-day-will-without-raising-ethical-concerns-or-leading-to-societal-disruption">
<h3>The text gives many examples of AI that quietly work well, like spellcheck. Can you think of other examples? What do you think are examples of tasks that AI can‚Äôt yet perform reliably but one day will, without raising ethical concerns or leading to societal disruption?<a class="headerlink" href="#the-text-gives-many-examples-of-ai-that-quietly-work-well-like-spellcheck-can-you-think-of-other-examples-what-do-you-think-are-examples-of-tasks-that-ai-cant-yet-perform-reliably-but-one-day-will-without-raising-ethical-concerns-or-leading-to-societal-disruption" title="Link to this heading">#</a></h3>
<p>Applications of AI that we could envision being less ethically concerning include scientific pursuits, such as animal conservation for tracking animals, biodiversity monitoring, birdsong recognition, weather prediction, pollution analysis, or finding biomarkers for diseases. These use cases do not directly dictate outcomes for people‚Äôs lives and can be verified by complementary scientific methods.</p>
<p>With respect to AI being used in everyday life, AI should act as a facilitator for human flourishing, rather than a supplement. We would much rather have AI do our laundry whilst we make art, rather than have AI generate ‚Äúart‚Äù whilst we do laundry. If ChatGPT was reliably accurate, we could imagine it being useful as a learning tool e.g. by quizzing students on their homework. Tools like Grammarly are great in certain contexts, because native English speakers can be harsh when assessing people‚Äôs writing. When people don‚Äôt have English as a first language, grammar correcting tools can help them to be sure they are saying the right things.</p>
<p>There are many AI tools we use every day without noticing, but that does not mean that they are working well. Spellcheck is integrated in many applications, however, we do not think it works well. If you write in more than one language, it falls apart and doesn‚Äôt understand what context it‚Äôs in. We tend to have counterintuitive expectations of the abilities of these tools; we recently encountered someone praising ChatGPT for being helpful with writing code about 60% of the time. If someone was asking us for help, and we were getting it wrong 40% of the time, we would not expect them to ask us for help again.</p>
<p>We would be hard pushed to find an AI tool that won‚Äôt raise ethical concerns or cause some disruption. If something could replace a job, it could cause disruption. There are lots of areas in which people don‚Äôt care how good the labour is, they just want an output ‚Äì even if it‚Äôs inaccurate generative AI nonsense. Even seemingly innocuous applications like spellcheck or Grammarly have rippling implications, e.g. for students. We have experienced spellcheck changing our answers leading to a quiz fail. If students can use something to do their work, it affects the work of teachers.</p>
<p>Once a tool can provide an answer it would take a human a while to come up with, it is easy to slip into <a class="reference internal" href="#www.mdpi.com/2075-4698/15/1/6"><span class="xref myst">cognitive offloading</span></a>. We‚Äôve noticed that tools like Grammarly are now pitched at native English speakers, potentially discouraging them from improving their own grammatical abilities. Lots of native English speakers who should already have these skills will defer to grammar checking tools, assuming that the tools will be better. People have <a class="reference external" href="https://thedecisionlab.com/biases/authority-bias">authority bias</a>, doubting their own knowledge because the tool has told them something different.</p>
<p>The disruptive potential of AI may depend on the application and sensitivity of the data. It is important to take into account the whole pipeline, considering not just what the tool does but the energy it consumes and where the data used to train it comes from.</p>
</section>
<section id="what-change-would-you-like-to-see-on-the-basis-of-this-piece-who-has-the-power-to-make-that-change">
<h3>What change would you like to see on the basis of this piece? Who has the power to make that change?<a class="headerlink" href="#what-change-would-you-like-to-see-on-the-basis-of-this-piece-who-has-the-power-to-make-that-change" title="Link to this heading">#</a></h3>
<p>Careful consideration of our attitudes towards AI is crucial in shaping the role it plays in our lives. It is difficult to maintain healthy scepticism of AI in the face of overwhelming hype. In general, the chapter sets a good tone, balancing criticisms with an awareness of potential benefits. Some of us have come to the book with a pro-AI attitude, looking to engage with it as a challenging view. Predictive and generative AI have beneficial use cases, e.g., generative AI can help people get started with a prototype for an idea even if they don‚Äôt know how to code. Others among us are looking to ask whether these benefits are worth the costs.</p>
<p>Some of us are becoming increasingly hardline anti-AI, finding that the chapter doesn‚Äôt go in as hard as it could (or maybe should) against AI. The chapter focuses, rightly, more on predictive AI, but it could have been more critical of generative AI. Fundamentally, many of us believe generative AI cannot do anything new. The outputs might be useful but are fundamentally unreliable in the sense that there is no validation of their correctness. The chapter talks about how ‚Äúfacial recognition works well enough to be harmful, and badly enough to be harmful‚Äù ‚Äì this probably applies to LLMs as well.</p>
<p>To highlight the problems with applying AI, we considered the difference between using facial recognition to block someone from attending an event, and having a bouncer at the door with a list of people not allowed in. The difference may lie with accountability; you can argue with a person or ask them to take action, but with an automated system you will get nowhere. A person might lose their job if they do it incorrectly; if a system makes a mistake, it does not pay for the consequences, and the costs of rollback are often too high to warrant a system change. A human security guard can‚Äôt be scaled up to surveil 100,000 people at once and get 5% wrong with no recourse.</p>
<p>Distinguishing between the technology itself and the people behind it is increasingly tricky, as the people behind it are <a class="reference external" href="https://dl.acm.org/doi/10.1145/3630106.3658543">increasingly distanced and invisibilised from the tool itself</a>.</p>
</section>
</section>
<section id="attendees">
<h2>Attendees<a class="headerlink" href="#attendees" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Huw Day, Data Scientist, University of Bristol: <a class="reference external" href="https://www.linkedin.com/in/huw-day/">LinkedIn</a>, <a class="reference external" href="https://bsky.app/profile/huwwday.bsky.social">BlueSky</a></p></li>
<li><p><a class="reference external" href="https://jessica-woodgate.github.io/">Jessica Woodgate</a>, PhD Student, University of Bristol</p></li>
<li><p>Liz Ing-Simmons, Research Software Engineer, King‚Äôs College London: <a class="reference external" href="https://genomic.social/&#64;liz__is">Mastodon</a> üë©‚Äçüíª</p></li>
<li><p>Vanessa Hanschke, Lecturer, University College London</p></li>
<li><p>Julie-M. Bourgognon, Lecturer in neurosciences, University of Glasgow</p></li>
<li><p>Euan Bennet, Lecturer, University of Glasgow: <a class="reference external" href="https://www.linkedin.com/in/euanbennet/">LinkedIn</a>, <a class="reference external" href="https://bsky.app/profile/dreuanbennet.bsky.social">BlueSky</a></p></li>
<li><p>Paul Matthews, Lecturer, UWE Bristol <a class="reference external" href="https://bsky.app/profile/paulusm.jellytussle.org">BlueSky</a>, <a class="reference external" href="https://scholar.social/&#64;paulusm">Mastodon</a></p></li>
</ul>
</section>
</section>
<section id="chapter-2-how-predictive-ai-goes-wrong">
<h1>Chapter 2 - How predictive AI goes wrong<a class="headerlink" href="#chapter-2-how-predictive-ai-goes-wrong" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>Chapter Summary<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>The chapter discusses the precedence of companies making strong claims made about the utility of automated decision-making systems using predictive AI in order to sell them. Models are claimed to be accurate, efficient (requiring little to no input from humans), and fair. One of the appeals of predictive AI is that it can reuse datasets that have already been collected for other purposes, such as record keeping or bureaucratic tasks. Models are also appealing through the promise of attempting to predict the future, as people struggle to deal with uncertainty or randomness and seek methods that enable control. These systems are used to allocate resources, provide or withhold opportunities, and predict peoples‚Äô future behaviour.</p>
<p>Yet, whilst a model may make a decision from an input without human involvement, human decisions still exist throughout the pipeline. Humans dictate the design of the model, the data that is collected, and the methods of deployment. It cannot be guaranteed that these decisions are unbiased or fair. Models tend to make predictions that are correct according to the way they were designed, but issues can arise in deployment. Important data may be missed or misunderstood, the system may change, or people may employ gaming strategies.</p>
<p>Once in place, systems are extremely hard to reverse, and people are unable to challenge decisions. Decision subjects are frequently unaware that they are being evaluated by AI, yet the decisions made can have life changing implications and mistakes are hard to fix. Even if human oversight is in place, it is often inadequate. Costs of flawed AI disproportionately harm groups that have been systematically excluded and disadvantaged in the past.
Instead of treating people as fixed and their outcomes as predetermined, the chapter argues that we need to accept the inherent randomness and uncertainty in life. Institutions should be built with an appreciation that the past does not predict the future.</p>
</section>
<section id="id2">
<h2>Discussion Summary<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<section id="predictive-models-make-common-sense-mistakes-that-people-would-catch-like-predicting-that-patients-with-asthma-have-a-lower-risk-of-developing-complications-from-pneumonia-as-discussed-in-the-chapter-what-if-anything-can-be-done-to-integrate-common-sense-error-checking-into-predictive-ai">
<h3>Predictive models make ‚Äúcommon sense‚Äù mistakes that people would catch, like predicting that patients with asthma have a lower risk of developing complications from pneumonia, as discussed in the chapter. What, if anything, can be done to integrate common-sense error checking into predictive AI?<a class="headerlink" href="#predictive-models-make-common-sense-mistakes-that-people-would-catch-like-predicting-that-patients-with-asthma-have-a-lower-risk-of-developing-complications-from-pneumonia-as-discussed-in-the-chapter-what-if-anything-can-be-done-to-integrate-common-sense-error-checking-into-predictive-ai" title="Link to this heading">#</a></h3>
<p>The idea of being able to model common sense is difficult to accept, as it is a changeable and contested concept. Common sense mistakes happen with or without AI, thus perhaps automated decisions are not necessarily worse than those made by humans. Models are a product of how they are trained; a model is only as good as the modeller. If what goes into a model is rubbish, you can expect that what comes out will be rubbish too: <a class="reference external" href="https://www.techtarget.com/searchsoftwarequality/definition/garbage-in-garbage-out">‚Äúgarbage in, garbage out‚Äù</a>. Algorithms are trained on data that reflects biases and prejudices held in society, such as racial inequities. We‚Äôve found evidence of this in <a class="reference external" href="https://arxiv.org/abs/2410.06385">research we‚Äôve conducted investigating the ability of a neural network to look for differences in skin tones in images of skin cancer lesions, finding that the network performed poorly</a>. In the case of detecting skin cancer lesions, the implications this has for detection rates for varying racial groups is troubling.</p>
<p>In addition to the data that goes into a model, the design of the model itself, such as the variables included, influences the predictions the model makes. It is crucial and difficult to select variables that are meaningful with respect to the question being asked. Identifying <a class="reference external" href="https://towardsdatascience.com/the-science-and-art-of-causality-part-1-5d6fb55b7a7c/">causal inference is hard</a>, and not every relationship you see in data is a causal relationship. Many AI models today have far too many variables to exhaustively check; <a class="reference external" href="https://explodingtopics.com/blog/gpt-parameters">ChatGPT-4 is estimated to have 1.8 trillion parameters</a>.</p>
<p>Considering the inherent difficulties with building common sense into a system, perhaps the best approach is to focus on employing common sense in the application of AI. To cultivate common sense application, the general perception of computational systems as infallible will need to change. People tend to treat AI systems as more knowledgeable than humans, thinking that computers don‚Äôt make mistakes and over-crediting their veracity. Humans transfer social trust onto machines, projecting an idea of ‚Äúexpertise‚Äù as systems appear to give confident answers on the base of some hidden knowledge. There may to be some correlation between size and scepticism; there tends to be more apprehension around small studies compared to larger ones, and similarly we are more likely to mistrust a smaller model compared to a massive one. However, just because something is big doesn‚Äôt mean it is right.</p>
<p>To properly evaluate a model, the broader context must be considered. Caution does not tend to be incorporated into models yet plays an important role in the way humans make and apply decisions. It is also important to distinguish between different sorts of problems, asking if the model itself is bad, or if the application of it is inappropriate. One should consider how the data was collected and what the shape of the problem looks like. For example, in predicting blood pressure, most people don‚Äôt have issues until they are older, creating a ‚ÄúU‚Äù shape in the model. Therefore, application of a linear model to this problem will not fit.</p>
<p>Incorporating human checks and oversight throughout the AI pipeline could help mitigate unwanted side effects. To avoid responsibility <a class="reference external" href="https://www.eversheds-sutherland.com/en/united-kingdom/insights/the-battle-against-corporate-washing">‚Äúwashing‚Äù</a>, wherein companies claim to have oversight without implementing proper procedures, oversight will need to be carefully defined including the likely failure modes to detect. Systems will need to be explainable so that overseers can understand what they are looking at. Perhaps legislation is one solution to require companies to explain the weaknesses of their models and highlight where oversight should be more closely applied, similar to how companies are required to list side effects for medication.</p>
<p>Statistical modelling is generally thought of as reliable, requiring deliberate choices which are made explicit. In machine learning (ML) contexts, sometimes those choices are not as transparent. There tends to be a lot of opacity in decision-making that goes into the development and application of models. To understand how black box models are working, the criteria that the model uses to make decisions should be pulled out. Sometimes, machines are making connections we would not know to make, or inferences that are not what they seem. For example, in healthcare applications, image classifiers have been found to be taking into account other elements of the image in their decision such as the use of rulers in an image.</p>
<p>If we are incapable of understanding a model, such as a cancer screener, we wondered whether or not we should be using it. It may not be essential to understand the mechanics of all the technology we use. Most of us use black box applications every day without question, such as computers, central heating, or aspirin.
Sometimes more transparency might not be appealing if revealing the criteria for decisions facilitates applicants gaming the system. In these settings there might be specific audiences for whom the system should be transparent to, e.g. the system should be transparent to hiring managers, but not to applicants. On the other hand, perhaps applicants would like to know the criteria they are being judged by, and withholding this information may be unfair.</p>
</section>
<section id="think-about-a-few-ways-people-game-decision-making-systems-in-their-day-to-day-life-what-are-ways-in-which-it-is-possible-to-game-predictive-ai-systems-but-not-human-led-decision-making-systems-would-the-types-of-gaming-you-identify-work-with-automated-decision-making-systems-that-do-not-use-ai">
<h3>Think about a few ways people ‚Äúgame‚Äù decision-making systems in their day-to-day life. What are ways in which it is possible to game predictive AI systems but not human-led decision making systems? Would the types of gaming you identify work with automated decision-making systems that do not use AI?<a class="headerlink" href="#think-about-a-few-ways-people-game-decision-making-systems-in-their-day-to-day-life-what-are-ways-in-which-it-is-possible-to-game-predictive-ai-systems-but-not-human-led-decision-making-systems-would-the-types-of-gaming-you-identify-work-with-automated-decision-making-systems-that-do-not-use-ai" title="Link to this heading">#</a></h3>
<p>There is an increasing sense for upcoming generations that to stay ahead they need to game systems, and those who don‚Äôt realise systems are gameable are put at a disadvantage. For example, understanding <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-05-22_writeup.html">how to answer personality tests</a> prevents <a class="reference external" href="https://dataethicsclub.com/write_ups/2025-03-05_writeup.html">job applicants from minority groups being screened out</a>. Part of the hacker mindset entails you are smart if you are able to hack things.</p>
<p>Even in non-AI contexts, there are plenty of examples of systems being gamed. People frequently game each other through manipulative tactics. Knowing what to say can get you the right hospital treatment or make you eligible for benefits in pre-screening processes. We wondered where the line is between tailoring appropriate communication to a particular audience and gaming a system. Perhaps there is some relation to whether one‚Äôs actions are working towards a particular desired outcome.</p>
<p>Settings in which people game predictive AI systems include <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-02-28_writeup.html">search engine optimisation, contributing to the enshittification of the internet by filling websites with algorithmic buzzwords</a>; job applications; social media. Industry resumes tend to be shorter than academic resumes, incentivising the use of buzzwords and listing skills like ‚Äúleadership‚Äù, ‚ÄúJava‚Äù, etc., that will bump the applicant up the list. To get your profile seen, we feel an increasing pressure to make our job applications and cover letters ‚ÄúLinkedIn friendly‚Äù. A funny side effect of this is that when someone says you are good at LinkedIn it feels like an insult by being seemingly inauthentic.</p>
<p>Buzzwords are also used to game funding or grant applications. We have seen examples of proposals suggesting a project will be using ‚ÄúAI‚Äù in its methods, where in reality it is not really an appropriate application of AI. Sometimes people want to use AI because it is trendy, whether or not it would actually solve the problem. Many people do not really understand what AI is, or the what the differences are between specific terms such as ML and AI (<a class="reference external" href="https://www.datacamp.com/blog/the-difference-between-ai-and-machine-learning">ML is a subfield of AI</a>). Being clear about terminology, especially in the mainstream, is complicated by <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-12-18_writeup.html">the hype cycle, which propagates uncertainty and sensationalist narratives</a>.</p>
<p>The hype cycle encourages people to adopt trending methods even if they do not have sufficient background or training to understand how the methods work, which is detrimental to the quality of research. Bad research deteriorates public trust in science and scientific outcomes. Interdisciplinary collaboration is essential to better support researchers, and perhaps the importance of various disciplines should be discussed at lower levels of education to foster this. <a class="reference external" href="https://www.scientificamerican.com/article/psychology-s-credibility-crisis-the-bad-the-good-and-the-ugly/">Psychology has suffered credibility crises</a>, but this has led to stronger research practices such as hypothesis pre-registration and more publishing of negative outcomes in journals.</p>
</section>
<section id="in-which-kinds-of-jobs-are-automated-hiring-tools-predominantly-used-how-does-adoption-vary-by-sector-income-level-and-seniority-what-explains-these-differences">
<h3>In which kinds of jobs are automated hiring tools predominantly used ? How does adoption vary by sector, income level, and seniority? What explains these differences?<a class="headerlink" href="#in-which-kinds-of-jobs-are-automated-hiring-tools-predominantly-used-how-does-adoption-vary-by-sector-income-level-and-seniority-what-explains-these-differences" title="Link to this heading">#</a></h3>
<p>We weren‚Äôt sure which fields automated hiring tools are currently being used in, so instead discussed the fields such tools might be best used in. Hiring tools could be helpful for positions with a high ratio of applicants, as sifting through thousands of applications is a time consuming and repetitive task. Another appropriate application for automated tools may be entry level jobs in which group interviews are already commonplace, to help speed up the process.</p>
</section>
<section id="id3">
<h3>What change would you like to see on the basis of this piece? Who has the power to make that change?<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>People gaming automated systems may change the systems themselves, depending on how and what the algorithms are learning. For instance, hiring algorithms may learn to favour people who figured out how to game them and thereby further incentivise those behaviours. Gaming the system can push things towards homogeneity, which we have seen in other applications such as TikTok, where monetisation depends on the length of engagement and so videos tend towards a minimum length.</p>
<p>As AI is adopted by both hirers and applicants, a feedback loop is formed where LLMs are used to write and apply for jobs, other AI systems sift through the applications, and each side learns to game the other. Increasing dependence on LLMs will lead to more mistakes: <a class="reference external" href="https://www.newscientist.com/article/2479545-ai-hallucinations-are-getting-worse-and-theyre-here-to-stay/">LLMs are producing less and less accurate results</a>, and are <a class="reference external" href="https://amandaguinzburg.substack.com/p/diabolus-ex-machina">shown to repeatedly hallucinate and backtrack</a>. It is important that society finds ways to resist slipping into homogeneity and error-strewn information as a consequence of LLM overuse.</p>
</section>
</section>
<section id="id4">
<h2>Attendees<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Huw Day, Data Scientist, University of Bristol: <a class="reference external" href="https://www.linkedin.com/in/huw-day/">LinkedIn</a>, <a class="reference external" href="https://bsky.app/profile/huwwday.bsky.social">BlueSky</a></p></li>
<li><p>Julie-M Bourgognon, Lecturer, University of Glasgow</p></li>
<li><p>Vanessa Hanschke, Lecturer, University College London</p></li>
<li><p>Paul Matthews, Lecturer, UWE Bristol ü¶£ https://scholar.social/&#64;paulusm</p></li>
<li><p>Liz Ing-Simmons, Research software engineer, King‚Äôs College London (my day: :hot_face: (it‚Äôs too hot here!)) | <a class="reference external" href="https://genomic.social/&#64;liz__is">Mastodon</a></p></li>
<li><p>Nicolas Gold, Associate Professor, UCL</p></li>
<li><p>Martin Donnelly, Principal Research Data Steward, UCL, martin.donnelly&#64;ucl.ac.uk (whatever the emoji for being embarassed at not having read the chapter yet is)</p></li>
<li><p>Robin Dasler, data product manager, California</p></li>
</ul>
</section>
</section>
<section id="chapter-3-why-cant-ai-predict-the-future">
<h1>Chapter 3 - Why can‚Äôt AI predict the future?<a class="headerlink" href="#chapter-3-why-cant-ai-predict-the-future" title="Link to this heading">#</a></h1>
<section id="id5">
<h2>Chapter Summary<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>The sheer number of correlations that certain AI methods can identify in data, and the usefulness of those methods in identifying which correlations are important, has contributed to the popularisation of AI to predict the future. Despite the efficacy of AI in identifying correlations, the world is profoundly complicated, and phenomena are often compounded by a myriad of variables. The weather, which humans have been trying to predict for thousands of years, is a good example of how easily real-world complexity interferes with prediction methods. In the 1960s, <a class="reference external" href="https://en.wikipedia.org/wiki/Edward_Norton_Lorenz">Edward Norton Lorenz</a> found that rounding the numbers of weather simulations to three decimal places instead of six gave vastly different results. This finding led to the coining of ‚Äú<a class="reference external" href="https://en.wikipedia.org/wiki/Butterfly_effect">the Butterfly Effect</a>‚Äù: the observation that small errors in measurement (e.g. of temperature) lead to exponentially increasing errors later. The farther away the prediction, the larger the error.</p>
<p>There are two main paradigms for predicting the future: simulation, and ML. Simulation is based on the idea that the future evolution of a system can be predicted using the current state of the system and equations describing how the system changes over time based on the interactions between its components. Simulations have proven useful for some applications, like the weather, and terrible at others, such as social phenomena like modelling a whole city. ML on the other hand uses past data to learn underlying patterns and make predictions about the future, without fixed rules about how the future will play out given the past. Predictions and patterns can adapt and change over time, but the ‚Äúrules‚Äù determining this are based on how the system behaved in the past. The chapter argues that simulation is more suitable for predicting things about collective or global outcomes, whereas ML is better suited to predict things about individuals. ML is appropriate in settings where there is a lot of data to train on, such as examples of spam or not spam emails, whereas simulation is appropriate where there is domain knowledge but not enough examples, such as food shortages.</p>
<p>The line between what is and isn‚Äôt feasible in prediction social phenomena is ill defined. It is too broad to state that we can‚Äôt predict any social phenomena, as some dynamics like traffic or how busy a store will be on a certain day can be predicted reasonably well. However, predicting a person‚Äôs future is hard because of a combination of the real-world utility of what can be done with the prediction, the moral legitimacy of the application, and the prediction‚Äôs irreducible error (error that won‚Äôt go away with more data and better models). Statistical or AI models used to predict life outcomes have repeatedly been shown to be little better than random, with complex models offering no substantial improvement above simple baseline models.</p>
<p>A problem in predicting the future is the difficulty in defining the accuracy of a prediction: the range of output that is considered accurate (e.g. for the weather, whether temperature is accurate within one degree or five degrees); the type of output (e.g. if it will reliably predict it will rain regardless of temperature); how the accuracy of one domain compares to another (e.g. weather patterns compared to sales patterns). To assess if a prediction task can be done well, there are qualitative criteria that can be examined, such as looking at what kinds of prediction tools people actually use, what can be done with a tool, relevant power dynamics and moral implications, and whether a prediction will improve with more data and better models.</p>
<p>It is not yet possible to conclude whether there are fundamental limits to predicting human lives, as has been proven in other domains such as planetary orbits or thermodynamics. We do not know what the irreducible error is for social prediction; it may be possible that predictability of life outcomes would be improved by the availability of more data. Yet, it seems likely that the irreducible error is high as chance events can drastically alter the path of a person‚Äôs life, either through large shocks or small (dis)advantages that compound over time. Patterns underlying social phenomena are not fixed and differ greatly based on context, time, and location. The sheer amount of data needed to predict life outcomes would be very high because social datasets have a lot of noise and the number of samples needed to create accurate models sharply increases as the samples become noisier. There are thus some limits to predicting the future that could be overcome with more and better data, whilst other limits seem intrinsic.</p>
</section>
<section id="id6">
<h2>Discussion Summary<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<section id="the-authors-list-3-main-criteria-for-good-prediction-use-cases-real-world-utility-moral-legitimacy-and-irreducible-error-error-that-wont-go-away-with-more-data-and-better-methods-is-this-list-complete">
<h3>The authors list 3 main criteria for good prediction use cases: real world utility, moral legitimacy, and irreducible error (error that won‚Äôt go away with more data and better methods). Is this list complete?<a class="headerlink" href="#the-authors-list-3-main-criteria-for-good-prediction-use-cases-real-world-utility-moral-legitimacy-and-irreducible-error-error-that-wont-go-away-with-more-data-and-better-methods-is-this-list-complete" title="Link to this heading">#</a></h3>
<p>To understand what makes a good use case for prediction, it is important for both scientists and non-scientists to be clear about the limits of probability. It is impossible to predict everything as probability entails there will always be a likelihood of <a class="reference external" href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positives</a> (when a result incorrectly indicates the presence of a condition). Scientists should be aware of the characteristics and implications of probability, but the awareness might not pervade in those without scientific education. The human intuition about probability is sometimes good, and sometimes terrible. We‚Äôve seen examples of the fallibility of intuition when teaching probability using loaded dice at science festivals, finding that children figure out the problem much faster than adults. The reasons behind this may be associated with ingrained expectations and assumptions held by adults which children are not exposed to.</p>
<p>As the rules of probability entail fundamental limits to prediction, a use case may be acceptable if the model fulfils criteria that makes it ‚Äúgood enough‚Äù and the consequences of incorrect predictions are not too severe. For example, weather prediction is good enough for most people if it roughly predicts the temperature, chance of rain, and other important features. If weather forecasts miss severe weather events, then their utility would be greatly reduced. A system may pass the ‚Äúgood enough‚Äù test if it can only be used in certain situations for predictions.</p>
<p>Prediction should be reasonably accurate, but defining what is reasonable is hard. All of the criteria the chapter discusses (real world utility, moral legitimacy, and irreducible error) are interlinked. The higher the moral stakes, the higher the need for accuracy and demand for lower irreducible error. Moral legitimacy includes aspects such as the impact on people‚Äôs lives, e.g. the impact of accusation or incarceration in the case of criminality prediction.</p>
<p>The impact of prediction on life outcomes draws attention to the fact that sometimes the prediction itself can change the outcome. If there is a really good way of predicting the future, the predictor might undermine themselves (the TV show <a class="reference external" href="https://en.wikipedia.org/wiki/Devs_(TV_series)">Devs</a> explores the spookiness of the relationship between prediction and free will). Because of the potential influence of prediction in changing outcomes, in some countries it is illegal to release polling data before elections as it could impact polling behaviour. On the other hand, there are situations where this is harnessed so that the purpose of the prediction is to instigate people to do something differently; the act of prediction does not thereby undermine the predictor. This however means you are no longer predicting the future, as you are making a prediction with the intention to stop it from happening.</p>
<p>The contradictory effects of prediction on changing outcomes made us wonder why polls are carried out in the first place; what the real world utility is and who they are intended to benefit. Sometimes, it is useful to know if the person in your seat can never win so that you can vote strategically, but in non-strategic voting systems this does not matter. There is a difference between polls used to tell people what the current voting preferences are, and <a class="reference external" href="https://en.wikipedia.org/wiki/Exit_poll">exit polls</a> which are taken immediately after voters have left the polling station to ask who they voted for. Exit polls are used as early indications for the outcome of the vote.</p>
<p>Some of the examples in the chapter seemed a bit contrived and some of us raised questions about the 8 billion problem (that we can‚Äôt make accurate social predictions because there aren‚Äôt enough people on earth to learn all the patterns that exist). There are some things that we can predict with a high amount of certainty, for example, I can predict that if I take a flight tomorrow, I‚Äôll probably land safely. Whether it is impossible for 8 billion people to make a dataset that is big enough may depend on what we are looking for. Often, we have enough data for people within the distribution we are looking at, but problems might arise with edge cases (e.g. rare diseases). For edge cases, we might have enough rows (i.e. participants) but not enough columns (i.e. features). GPs in the UK use <a class="reference external" href="https://www.qrisk.org/">QRISK to predict heart attacks</a>, which was developed only on several hundred people. Improvements on QRISK may come from the addition of more features.</p>
</section>
<section id="cumulative-advantage-implies-a-lot-of-success-comes-down-to-luck-which-challenges-a-meritocratic-world-view-how-do-you-feel-about-this">
<h3>Cumulative advantage implies a lot of success comes down to luck, which challenges a meritocratic world view. How do you feel about this?<a class="headerlink" href="#cumulative-advantage-implies-a-lot-of-success-comes-down-to-luck-which-challenges-a-meritocratic-world-view-how-do-you-feel-about-this" title="Link to this heading">#</a></h3>
<p>Putting too much emphasis on luck can be detrimental; we don‚Äôt want the takeaway to be that everything is luck so there is no point in trying. It‚Äôs still important to strive for things we think or know are impossible, as incremental progress can be made. Some of us thought that we should strive for more meritocracy because of this. We wondered if we would be able to predict success better if we lived in a more meritocratic society. Overemphasis on meritocracy may however run into the problem of predicting and thereby pre-empting success, promoting those who are predicted to be successful rather than those that have proven to be successful. One of the main <a class="reference external" href="https://press.princeton.edu/ideas/a-belief-in-meritocracy-is-not-only-false-its-bad-for-you">criticisms of meritocracy</a> is that it ends up supporting those who already have an unfair advantage due to societal power dynamics.</p>
<p>Cumulative advantage is something that we have observed in our day-to-day life such as by clicking more on the most downloaded songs and other demonstrations of herd behaviour. For music and movies there are other factors that intertwine with herd behaviour and may factor into cumulative advantage such as the effect of nostalgia and role of cultural background.</p>
</section>
<section id="social-sciences-focus-on-understanding-causal-mechanisms-not-predicting-associations-as-opposed-to-typical-machine-learners-what-do-both-fields-have-to-learn-from-one-another">
<h3>Social sciences focus on understanding causal mechanisms, not predicting associations as opposed to typical machine learners. What do both fields have to learn from one another?<a class="headerlink" href="#social-sciences-focus-on-understanding-causal-mechanisms-not-predicting-associations-as-opposed-to-typical-machine-learners-what-do-both-fields-have-to-learn-from-one-another" title="Link to this heading">#</a></h3>
<p>Traditional statistical models are based on explicit mathematical equations that can be interpreted by experts and explained to others. In comparison, understanding how a neural network makes a particular decision is very difficult. The connections the network learns between data are masked by the copious number of parameters involved, making it difficult to draw out what exactly happens within a model and how it is handling the data. This intrinsic implicitness complicates identifying how best to interpret the model and understand how the outputs are produced from the inputs.</p>
<p>Causality is a murky question and a philosophical rabbit hole, so there is a tendency among statisticians to avoid it. Yet, if we predict what will happen based on what has already happened, we will propagate and amplify historical issues into the future. Instead of using the past and all the baggage it comes with, predictions regarding human lives should place more emphasis on causality.</p>
<p>Attempts to understand causality must be accompanied by domain specific knowledge and a scientific mindset. A good example of the success of domain knowledge and scientific method coming together is the true story that was adapted into the film <a class="reference external" href="https://en.wikipedia.org/wiki/Moneyball_(film)">Moneyball</a>. In Moneyball, baseball pundits had bad judgments about what made good players. Billy Beane, the manager of the Oakland Athletics baseball team, hired a Yale economics graduate and together they were able to derive what made players valuable and build a successful team.</p>
<p>ML models are very good at identifying patterns, but the utility of those patterns depletes if they are detached from domain expertise and we are unable to understand how the patterns are made. Domain expertise is essential to pursue actually useful models; without domain expertise to define what exactly we are looking for before starting the modelling process, there is little chance of finding useful patterns. In ML development there seems to be a tendency to chuck a bunch of data into black box models that the developers can‚Äôt explain, rather than putting in the work to incorporate domain expertise. The mystery that is so embedded in this process lends favour to the image of ‚Äútech bros‚Äù who are smart at STEM, so people assume they are good at everything else. This characterisation is bad ‚Äì it stops people from questioning and criticising them, enabling bad decision-making and unaccountability.</p>
<p>As more and more information becomes available to us, we need more and more time to become experts. It is crucial to recognise expertise across fields and support experts in communicating with each other. Some of us wondered if this is an opening for generative AI to emulate expertise across an expanse of fields. Others worried that the use of generative AI here would mean skipping a communication step, and perhaps it would be more useful to use it to augment and support communication rather than replace it. If generative AI is used in replacement, we wondered if there is a point in proving a theorem if no-one can understand or replicate it. However, generative AI surpassing human ability in certain settings might not be important if the application of the technology furthers human knowledge; there are cases where geniuses like Stephen Hawking have proven something that nobody else could have.</p>
<p>To delineate (in)appropriate uses for generative AI we need to be clear about what our goal is; whether we want to learn something, or whether we just want a task to be completed. For some of us, proving something that no humans can understand is less interesting even if we don‚Äôt think it‚Äôs less valuable. At the very least, someone should be able to understand it. In education settings, there is too often an overemphasis on doing assignments and turning them in, rather than spending the effort to truly learn the lesson underpinning the assignment.</p>
</section>
<section id="id7">
<h3>What change would you like to see on the basis of this piece? Who has the power to make that change?<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>Limitations to ML methods entail that when approaching a problem, it is important to ask if ML is actually the appropriate tool to use, or if there is a faster and cheaper way. In the book, the examples of successful predictive models were interpretable mathematical models and the ML examples either failed or were worse than ‚Äúsimpler‚Äù models. We wondered if there are any examples of ML models that work, and work better than simpler models. The lack of examples of successful ML may be because <a class="reference external" href="https://machinelearningmodels.org/the-evolution-of-machine-learning-a-brief-history-and-timeline/">ML is a (relatively) young field</a>. ML may become increasingly better than simpler models as the field matures.</p>
</section>
</section>
<section id="id8">
<h2>Attendees<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Huw Day, Data Scientist, University of Bristol: <a class="reference external" href="https://www.linkedin.com/in/huw-day/">LinkedIn</a>, <a class="reference external" href="https://bsky.app/profile/huwwday.bsky.social">BlueSky</a></p></li>
<li><p><a class="reference external" href="https://jessica-woodgate.github.io/">Jessica Woodgate</a>, PhD Student, University of Bristol</p></li>
<li><p>Euan Bennet, Lecturer, University of Glasgow, <a class="reference external" href="https://www.linkedin.com/in/euanbennet/">LinkedIn</a>, <a class="reference external" href="https://bsky.app/profile/dreuanbennet.bsky.social">BlueSky</a></p></li>
<li><p>Virginia Scarlett, Open Source Programs Specialist, UC Santa Barbara</p></li>
<li><p><a class="reference external" href="https://linkedin.com/in/robindasler">Robin Dasler</a>, data product manager on hiatus, California
-Julie-M. Bourgognon, lecturer, University of Glasgow</p></li>
</ul>
</section>
</section>
<section id="chapter-4-the-long-road-to-generative-ai">
<h1>Chapter 4 - The Long Road to Generative AI<a class="headerlink" href="#chapter-4-the-long-road-to-generative-ai" title="Link to this heading">#</a></h1>
<section id="id9">
<h2>Chapter Summary<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<p>Generative AI comes behind a long history of computational innovations. From the development of the <a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a> in the 1940s, based on a mathematical model of a neuron, to the boom and busts of neural networks over the latter half of the 20th century, eventually bringing us where we are today with <a class="reference external" href="https://news.mit.edu/2023/explained-generative-ai-1109">massive neural networks that can generate diverse outputs from text, to images, to audio, and more</a>. The chapter argues that generative AI can be hugely beneficial for a range of use cases from assisting knowledge workers, who ‚Äúthink for a living‚Äù, to improving accessibility such as for the partially sighted.</p>
<p>One application that deep neural networks have proven really effective in is image classification. Historically, labelling and classifying images required a marathon of human effort. Today, it is something that can be done in seconds using AI. Building on the success of neural networks, over recent years generative AI has received an escalating level of attention. There is a huge community of people working on the technology, contributing to its application in an ever increasing expanse of use cases and rapid evolution of the technology itself.</p>
<p>As we see time and time again, alongside excitement for a new AI technology comes a range of harms. The rapid uptake of a technology before it is properly understood and potential harms have been rigorously studied opens the door for destructive consequences. Generative AI has produced outputs that are <a class="reference external" href="https://www.bloomberg.com/graphics/2023-generative-ai-bias/">racist and inappropriate</a>,  <a class="reference external" href="https://www.technologyreview.com/2022/12/13/1064810/how-it-feels-to-be-sexually-objectified-by-an-ai/">oversexualised</a> or <a class="reference external" href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html">claiming to be sentient</a>, declared <a class="reference external" href="https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/">sentient by prominent engineers</a>, been <a class="reference external" href="https://www.lawcareers.net/Explore/News/High-Court-warns-lawyers-over-AI-misuse-after-fake-case-law-citations-11062025">used in real law cases to generate citations proven fake</a>, <a class="reference external" href="https://www.vice.com/en/article/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says/">associated with suicides when used as companion bots</a>, and used for <a class="reference external" href="https://www.forbes.com/sites/rashishrivastava/2023/05/11/reddit-ai-generated-porn/">non-consensual porn deepfakes</a>. Copyright issues have led to <a class="reference external" href="https://cms.law/en/int/publication/artificial-intelligence-and-copyright-case-tracker">a number of legal cases currently passing through the courts</a> concerning the appropriation of artists‚Äô work to train models. Models do not just adapt the works they have been trained on to produce new outputs, but in some cases can almost exactly <a class="reference external" href="https://www.analyticsinsight.net/artificial-intelligence/ai-image-tools-can-reproduce-the-iconic-mona-lisa-but-artists-are-hating-it">replicate instances of their training data, such as the Mona Lisa</a>.</p>
<p>After training LLMs on copious amounts of data, additional <a class="reference external" href="https://www.turing.com/resources/finetuning-large-language-models">fine tuning</a> has led to highly convincing chatbots which are good at producing believable responses but lack real-world grounding and verification. On a practical level, chatbots are at their core statistical engines. They have incomplete internal representations of the world around them and thus lack true ‚Äúunderstanding‚Äù. LLMs have <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-09-25_writeup.html">a tendency to produce ‚Äúbullshit‚Äù</a>, which are outputs that attempt to persuade without regard for the truth. Researchers‚Äô understanding of these internal representations is still rudimentary due to the lack of interpretability in what neural networks encode. We do know, however, that because LLMs learn from historical data, they will also learn any historical biases contained within that data. Attempts to mitigate these harms come in the form of more <a class="reference external" href="https://huggingface.co/blog/rlhf">human annotation</a>, which raises more issues with worker rights as the labour tends to be outsourced to poorer countries than where the company is located so that <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-05-08_writeup.html">they can capitalise on low wages and less regulation</a>.</p>
</section>
<section id="id10">
<h2>Discussion Summary<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<section id="various-ai-systems-such-as-the-neural-networks-in-the-imagenet-competitions-are-designed-to-automate-something-that-is-expensive-to-do-on-mass-e-g-labelling-images-and-where-the-process-in-doing-so-is-not-valuable-to-the-humans-doing-it-what-tasks-do-you-see-genai-both-llms-and-image-generators-being-used-for-and-how-much-are-they-being-used-for-processes-where-there-is-value-for-humans-performing-it">
<h3>Various AI systems, such as the neural networks in the ImageNet competitions, are designed to automate something that is expensive to do on mass (e.g. labelling images) and where the process in doing so is not valuable to the humans doing it. What tasks do you see GenAI (both LLMs and image generators) being used for and how much are they being used for processes where there is value for humans performing it?<a class="headerlink" href="#various-ai-systems-such-as-the-neural-networks-in-the-imagenet-competitions-are-designed-to-automate-something-that-is-expensive-to-do-on-mass-e-g-labelling-images-and-where-the-process-in-doing-so-is-not-valuable-to-the-humans-doing-it-what-tasks-do-you-see-genai-both-llms-and-image-generators-being-used-for-and-how-much-are-they-being-used-for-processes-where-there-is-value-for-humans-performing-it" title="Link to this heading">#</a></h3>
<p>Many tech leaders are loudly proclaiming the promise of generative AI to automate work as we witness <a class="reference external" href="https://tech.co/news/companies-replace-workers-with-ai">a wave of mass firings and human workers being replaced by AI</a> (explored in a <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-01-31_writeup.html">previous Data Ethics Club</a>). Generative AI proponents advocate that the technology will <a class="reference external" href="https://hbr.org/2025/02/im-afraid-we-are-automating-this-work-without-really-understanding-it">free humans up for meaningful work</a>. The implication of these messages is that the work people are attempting to replace with AI, including some of our jobs or things that we do in our jobs, are not meaningful.</p>
<p>Defining what it really means for something to be valuable to humans is critical to ensuring that generative AI is used in ways that truly benefit society and yet seems to be often brushed aside. The current landscape of generative AI tools tends to implement them in obvious applications that the tools can speed up, such as writing emails, coding, or labelling images, but without delving deeper to ask questions about if these are the sorts of things that we actually should be using it for. If we want to use AI to label people in photographs, who is that valuable to, and why is it valuable? Many tools have been proposed as a solution for efficiency, without critically assessing whether there is more besides efficiency that needs consideration.</p>
<p>Perhaps generative AI should be used for doing tasks that nobody wants to do and that we would rather a machine do than a human. If people do not understand or appreciate the effort required for a task, perhaps it is not worthwhile putting a lot of time into it, and we should use whatever means available to us to expediate it. An application we have seen for computer vision that seems intrinsically valuable to us is medical imaging, where previously we worked on teams with many people spending hours labelling images. Many of those labelling images found the task to be boring, and did not leave them room to grow through their jobs. Today, AI demonstrates high utility for completing these sorts of tasks quick and at high volume, leading to significant scientific contributions such as <a class="reference external" href="https://www.nature.com/articles/d41586-024-03190-y">creating a complete map of the neurons in a fly‚Äôs brain</a> and <a class="reference external" href="https://ccr.cancer.gov/news/article/new-ai-tool-classifies-brain-tumors-using-images-of-tumor-slides">tumour identification</a>.</p>
<p>Other touted applications of AI may not be as appealing as they seem when we delve deeper into them. We have conducted work anonymising text data about medical conditions, which is a task that could arguably be automated by AI with advantages for efficiency. However, whilst we found the experience to be quite heartbreaking it also really motivated us to work on the project and helped us appreciate the stories within the data. We wouldn‚Äôt want to lose this connection with stories; if you just deal with the summary level you do not get same the richness as reading the actual interviews. These experiences highlight how there is value that lies in painstaking qualitative data analysis. Some of us think that a hybrid approach could work, where generative AI can be used for qualitative data analysis but still needs to be checked.</p>
<p>Concerns with the trustworthiness of generative AI gives us reason to be cautious in our use of it and question how valuable it is for day-to-day use. A lot of the marketing surrounding generative AI tools centres around <a class="reference external" href="https://journals.stmjournals.com/joaira/article=2024/view=191587/">automating admin tasks</a>, but we were unsure if we would trust an AI agent to perform these tasks correctly and in privacy preserving ways. It may take a significant amount of effort and careful wording to prompt an AI in such a way that it completes your requests truly according to your specifications.</p>
<p>There is an important distinction between the value of the product and the value of the process. Something that is valuable to do intrinsically is not the same as something that is valuable by its outcome. Sometimes the process itself is valuable, such as in making art. Often artists are admired for the effort that goes into creating art. The hype narrative centring on using AI to automate ‚Äúboring‚Äù tasks is a narrow view that just focusses on speeding up the process to get to the product faster, rather than thinking holistically about the whole pipeline. Using AI to write code has become very popular, with many users just wanting something that runs without error even if it does not evaluate the right answer. A lot of students and staff we encounter use LLMs like they‚Äôre a search engine and expect the output to always be correct. Outsourcing the search strategy to an LLM shortcuts what could be a valuable process of evaluating sources and their utility. Many people don‚Äôt know this process exists, let alone recognise its value. Searching and analysing sources is a critical skill taught by librarians that should be more highly recognised.</p>
<p>A question that seems to have been skimmed over by the AI hype narrative is whether we really want everything to be maximally efficient. There is an appeal to doing mundane work and many of us want some parts of our jobs to be boring. We can‚Äôt be turned on and 100% high functioning all the time, and there are other things that matter to us in life aside from work which we want to save energy for. <a class="reference external" href="https://www.bbc.co.uk/future/article/20141218-why-boredom-is-good-for-you">Getting bored gives space for creativity to flourish, to get curious, and find meaning in life</a>. Monotony also provides opportunities to socialise. We have had jobs doing repetitive factory work but found it to be very social as we chatted with the people around us while we worked. Across many sectors of society more and more distance is being created between people, disincentivising interaction with others. In our offices people used to spend more time standing around the water fountain chatting. Shopping used to involve talking to someone at the checkout and someone helping you pack your bags but now that <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-05-08_writeup.html#what-do-you-think-of-the-various-types-of-automation-in-grocery-stores-are-they-convenient-desirable-or-worse-for-the-shopping-experience-do-you-make-use-of-them">checkouts are automated</a> it is harder and harder to talk to an actual live person. Sometimes when interacting with a company you don‚Äôt even know if you are interacting with a human or a bot. We‚Äôve had interactions that seemed like we were talking to a bot but turned out to actually be people in India.</p>
<p>Throughout our lifetimes, we have seen many fads come and go. For example, there has been <a class="reference external" href="https://thesurgicalclinics.com/history-of-robot-assisted-surgery/">excitement about robots performing surgery since the 1970s</a> yet we are still operated on primarily by humans. The extent of the hype and disruption around AI, however, makes us wonder if it is a craze that will not tail off. Hype cycles are problematic (explored in a <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-12-18_writeup.html">previous Data Ethics Club</a>). One of the many issues is that hype generates over trust in machines leading to technologies not being properly checked. Consequences of improper checking can be life changing for people, such as the <a class="reference external" href="https://www.bbc.co.uk/news/articles/c1wpp4w14pqo">Post Office scandal in the UK</a> where hundreds of people were prosecuted because of faulty accounting software. Rather than investigate and fix the problems with the software itself, the Post Office blamed branch operators for financial discrepancies.</p>
<p>It is also important to acknowledge that it is possible to automate systems without using AI, such as using statistical models for medical imaging versus AI and ML techniques. For medical imaging, ML methods do not address causation which is central to identifying diseases. If we are using ML for these sorts of applications, then proper checks and balances are indispensable.</p>
<p>In automating processes and evaluating models we must be clear about how we verify what is considered good or bad. This is easier in some domains than others. For example, if we know what a good scientific image diagram looks like, where there are well-defined rules and criteria, we can evaluate whether an AI has done a good job in producing one. It is harder to evaluate whether an AI summary of a long article is good if we haven‚Äôt read the original article or know the domain in depth. Some of us wondered if perhaps no-one without a PhD should be using certain AI tools, and outputs should be checked by multiple people with PhDs. Similar to how new drugs are evaluated in pharmaceuticals, all consequences and side effects should be considered. In AI settings, this includes bias and institutional and historical power dynamics; as ML relies on and continues to propagate the past, issues of the past will percolate into the future if unmitigated.</p>
<p>Our discussions make some of us yearn for a simple formula to work out if something is a good use case for AI. Important questions to ask would include how boring the work is, how verifiable the results are, how risky the setting is, whether there is enough data to make an accurate prediction, and whether it removes the accountability for a decision from a human. These are all essential considerations but must be held alongside the knowledge that there is often not a simple answer and ethics is grey.</p>
</section>
<section id="generative-ai-is-built-using-the-creative-output-of-journalists-writers-photographers-artists-and-others-generally-without-consent-credit-or-compensation-discuss-the-ethics-of-this-practice-how-can-those-who-want-to-change-the-system-go-about-doing-so-can-the-market-solve-the-problem-such-as-through-licensing-agreements-between-publishers-and-ai-companies-what-about-copyright-law-either-interpreting-existing-law-or-by-updating-it-what-other-policy-interventions-might-be-helpful">
<h3>Generative AI is built using the creative output of journalists, writers, photographers, artists, and others ‚Äî generally without consent, credit, or compensation. Discuss the ethics of this practice. How can those who want to change the system go about doing so? Can the market solve the problem, such as through licensing agreements between publishers and AI companies? What about copyright law ‚Äî either interpreting existing law or by updating it? What other policy interventions might be helpful?<a class="headerlink" href="#generative-ai-is-built-using-the-creative-output-of-journalists-writers-photographers-artists-and-others-generally-without-consent-credit-or-compensation-discuss-the-ethics-of-this-practice-how-can-those-who-want-to-change-the-system-go-about-doing-so-can-the-market-solve-the-problem-such-as-through-licensing-agreements-between-publishers-and-ai-companies-what-about-copyright-law-either-interpreting-existing-law-or-by-updating-it-what-other-policy-interventions-might-be-helpful" title="Link to this heading">#</a></h3>
<p>Our legal systems are currently in the midst of working out how to address copyright in light of the new AI landscape. There are lessons from the past that could be useful here; the power dynamics of the tech industry with a handful of titans dominating the market draws more than a few resemblances to <a class="reference external" href="https://www.ft.com/content/6f28e62a-d97d-49a6-ac3b-6b14d532876d">railroad companies in the US towards the end of the 1800s</a>. Railroads were deemed to be monopolies, leading to <a class="reference external" href="https://generisonline.com/the-evolution-of-antitrust-laws-historical-context-and-current-trends/">the introduction of antitrust leglislation</a>. Antitrust legislation <a class="reference external" href="https://www.investopedia.com/insights/history-of-us-monopolies/">banned monopolies that placed unreasonable restrictions on trade, which enabled the federal government to break up big companies</a>. Whether antitrust legislation really worked is debatable, but perhaps some similarly disruptive legislation is long overdue for big tech companies (<a class="reference external" href="https://www.cbc.ca/listen/cbc-podcasts/1353-understood/episode/16142603-introducing-understood-who-broke-the-internet">this podcast</a> gives a perspective on how we got to where we are today by <a class="reference external" href="https://en.wikipedia.org/wiki/Enshittification">Cory Doctorow, who coined the term ‚ÄúEnshittification‚Äù</a>, explored in <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-02-28_writeup.html">a previous Data Ethics Club</a>).</p>
<p>A case regarding <a class="reference external" href="https://www.ft.com/content/6f28e62a-d97d-49a6-ac3b-6b14d532876d">Meta‚Äôs use of millions of books to train AI systems has recently been deemed fair use</a>. Our reading of the outcome is that using art to train AI was deemed fair use, but pirating art to train AI was not. District judge Vince Chhabria noted in his decision that the ruling does not entail the use of copyrighted materials to train LLMs is lawful, but that the plaintiffs made the wrong arguments. Chhabria suggested that a potentially winning argument in the case would be that Meta had caused market dilution by flooding the market with endless amounts of content thereby causing damage to copyright holders.</p>
<p>Companies profiting off of artists‚Äô work without their consent seems intrinsically bad to us, but it might be too late to turn back the clock on how wide we allow the reach of big companies to be. There is an inequality between the rich and everyone else in how copyright is enforced. <a class="reference external" href="https://www.vice.com/en/article/music-piracy-bootlegging/">Ordinary people, including a 12 year old girl, have been prosecuted for sharing MP3 tracks</a> yet today generative AI companies are seemingly getting off scot-free for using millions of songs, images, books, and more ‚Äì and making money from it.</p>
<p>One imagination for after the court cases have been worked out is that the market for artists will look more similar to something like Spotify or iTunes, where artists are compensated a little bit but not much. This idea is
believable but perhaps not desirable; the monetisation structure of these platforms means artists have to be fairly large to earn real money and it is almost impossible for smaller artists to make a living.</p>
<p>To redress power imbalances, perhaps companies that profit off of AI trained on other people‚Äôs data or creative works should be required to repay the public, as explored in the idea of <a class="reference external" href="https://dataethicsclub.com/write_ups/2023-11-08_writeup.html">data labour</a>. Some of us think that either we acknowledge that data or creative works were given freely and consensually, or they were stolen. While we have been taking photographs and posting them on Facebook over the last 20 years, we have been doing so to share them with our friends; we did not think that the photos could be taken and used for other purposes. The publicness of the internet is a consequence of the mechanisms by which we can share stuff. Yet, artists posting their portfolios online are doing so for employment opportunities, not to condone the misuse of their work for others to make money. We do not think other people should profit from our work, which is why some of us who write poetry never post it online. We want our work to be out there and free for people to appreciate, but we don‚Äôt want it to be appropriated and fed into AI. Taking our work to train AI feels like our creativity being stolen.</p>
<p>It should be much easier to decline your data being given away and used to train AI models in order to protect peoples‚Äô autonomy. Currently, it is nearly impossible to not use AI products such as on our phones or browsers, and it is not clear how those AI products handle our data. Similar issues crop up with facial recognition compared to DNA swabs. In the US, <a class="reference external" href="https://www.rothdavies.com/criminal-defense/frequently-asked-questions-about-criminal-defense/searches/can-prosecutors-use-dna-taken-without-consent-or-a-warrant/">police need a warrant to collect DNA swabs</a> yet <a class="reference external" href="https://news.bloomberglaw.com/privacy-and-data-security/facial-recognition-software-is-everywhere-with-few-legal-limits">facial recognition has much lower restrictions and is increasingly ubiquitous</a>.
With many of the big players in tech being US based, the perspective on privacy is highly US centric. However, views on privacy and expectations of public observation differ across the world. <a class="reference external" href="https://www.androidpolice.com/google-street-view-germany-after-10-years-privacy-outcry/">Germany restricted Google Streetview until 2023</a> as people weren‚Äôt comfortable with pictures of their houses being online. The presumption that if something is on the internet, it‚Äôs fair game, is flawed and does not align with global privacy preferences. Despite this, we must acknowledge that this view is the default and do our best to raise the importance of other views.</p>
<p>The assumption that posting something online removes your ownership of it also has implications for responsibility and accountability. When the systems trained on public information cause harm, distance from those whose information was used to train the system, as well as the developers and deployers of the system itself, makes it much more difficult to attribute accountability. In our workplaces, we see a lack of accountability in the way people use LLMs for daily tasks and try to discourage this by picking people up on using generative AI to write emails or generate images.</p>
<p>Given that most of the content we post online is spread across multiple different platforms, the way our data is used and any attempts at compensation will depend on each platform‚Äôs terms of service. Under some circumstances, perhaps payment should go through the platform itself. <a class="reference external" href="https://www.deviantart.com/">DeviantArt</a> is an example of a platform that has <a class="reference external" href="https://slate.com/technology/2024/05/deviantart-what-happened-ai-decline-lawsuit-stability.html">fallen prey to the boom of generative AI</a> through a plague of bots and failure to protect users. The events surrounding DeviantArt made us wonder how outspoken users need to be when a platform changes its terms of services, and if they should have to speak up every time. It seems unfair to place the burden of holding platforms accountable on the users, however, it‚Äôs important that users do not become desensitised to changes in terms of service. <a class="reference external" href="https://observablehq.com/&#64;ayhanfuat/the-fall-of-stack-overflow">Stack Overflow has also seen a drop off in users</a> coinciding with the rise in AI coding assistants on top of <a class="reference external" href="https://www.webdesignerdepot.com/the-decline-of-stack-overflow-where-are-developers-headed-next/">growing dissent regarding the perceived culture of elitism</a>.</p>
</section>
<section id="discuss-the-environmental-impact-of-generative-ai-what-if-anything-is-distinct-about-ais-environmental-impact-compared-to-computing-in-general-or-other-specific-digital-technologies-with-a-large-energy-use-such-as-cryptocurrency">
<h3>Discuss the environmental impact of generative AI. What, if anything, is distinct about AI‚Äôs environmental impact compared to computing in general or other specific digital technologies with a large energy use such as cryptocurrency?<a class="headerlink" href="#discuss-the-environmental-impact-of-generative-ai-what-if-anything-is-distinct-about-ais-environmental-impact-compared-to-computing-in-general-or-other-specific-digital-technologies-with-a-large-energy-use-such-as-cryptocurrency" title="Link to this heading">#</a></h3>
<p>An important distinction between generative AI and other energy intensive technologies, such as mining cryptocurrency or running complicated physics simulations, is that generative AI is extremely easy to use. The ease of access prevents people from thinking about environmental concerns; because it‚Äôs easy to prompt, the impression is given that it‚Äôs computationally easy to ‚Äúdo‚Äù. Many people see computers as magic boxes that do things and are not aware of their computational costs. Even for technical people it‚Äôs hard to find information about the environmental impact of computers, especially regarding the emissions associated with manufacturing. For example, manufacturers do not provide information about the impact of GPUs compared to CPUs. It‚Äôs important to widen awareness of the mechanics behind AI tools, how they work, and what they‚Äôre good and bad at. Equipping people with this knowledge helps them to be more selective in their use, so that instead of jumping straight to an LLM they first use a calculator or Wikipedia.</p>
</section>
</section>
</section>
<section id="chapter-5-is-advanced-ai-an-existential-threat">
<h1>Chapter 5 - Is Advanced AI an Existential Threat?<a class="headerlink" href="#chapter-5-is-advanced-ai-an-existential-threat" title="Link to this heading">#</a></h1>
<section id="id11">
<h2>Chapter Summary<a class="headerlink" href="#id11" title="Link to this heading">#</a></h2>
<p>Artificial General Intelligence (AGI) is an ill-defined term used to describe the stage of AI development represents a threshold for large-scale risks becoming serious. As AGI is a philosophically contentious concept, the chapter adopts the definition of AGI as ‚ÄúAI that can perform most or all economically relevant tasks as effectively as any human‚Äù. This definition, the chapter argues, bypasses the need to delve into questions about subjectivity and consciousness.</p>
<p>Stories about advanced AI have captured our imagination over and over again in books (<a class="reference external" href="https://en.wikipedia.org/wiki/Foundation_(novel_series)">Asimov‚Äôs Foundation series</a>), films (<a class="reference external" href="https://en.wikipedia.org/wiki/2001:_A_Space_Odyssey">2001: A Space Odyssey</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/The_Terminator">The Terminator</a>), and even ancient mythology of man-made artificial beings (<a class="reference external" href="https://en.wikipedia.org/wiki/Golem">Golem</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Talos">Talos</a>). Today, the presence of AI in everyday life combined with generative AI‚Äôs quasi-realistic depiction of sentience narrows the gap between those imagined worlds and our own experiences.</p>
<p>Many prominent figures in tech lean into the science-fiction narratives surrounding AI, with some claiming to be <a class="reference external" href="https://blog.samaltman.com/the-gentle-singularity">directly working towards ‚Äúsuperintelligence‚Äù</a> and others voicing concerns about AI <a class="reference external" href="https://edition.cnn.com/2023/03/29/tech/ai-letter-elon-musk-tech-leaders">causing catastrophic harm</a>. Researchers concerned about existential risks of AGI argue that it should be a global priority alongside threats such as pandemics and nuclear war. Yet, arguments for existential threats of AGI are based on a tower of fallacies. Reasons for the prevalence of these false narratives could be due to selection bias, where people are drawn to AI research exactly because of the prospect of building an all-powerful technology, and cognitive bias, wherein the idea of imminent and terrifying AGI adds an aura of grandeur to one‚Äôs work.</p>
<p>Whilst AI has become increasingly generalised over the years, at each stage it has proven difficult to tell whether the current dominant paradigm can be further generalised or if it is a dead end. The latest rung in the ladder of generality is the use of LLMs to specify a computational task without having to do any programming. This step has turned AI into a consumer tool, making it accessible to people regardless of their understanding of programming or computation.</p>
<p>Whilst advances in generative AI have facilitated a jump in generalisability, the history of AI research has shown us that once one aspect is automated, other aspects previously unrecognised tend to reveal themselves as bottlenecks. Automating the whole pipeline of programming, to the extent that AI can program itself, seems unlikely without a colossal amount more data than is currently available. The gap between chatbots and embodiment is also significant, with robotics <a class="reference external" href="https://advanced-intelligent-robotics.com/what-are-the-limitations-of-current-robotics-technology/">facing several massive obstacles</a>. Even if it were possible to create AI that is truly intelligent, it seems unlikely to go ‚Äúrogue‚Äù. There is so much context to consider in catastrophic scenarios like <a class="reference external" href="https://en.wikipedia.org/wiki/Instrumental_convergence">turning the whole world into paperclips</a> that the context under consideration probably contains enough information for the AI to identify that it is a bad idea. If an AI were to attempt to take over the world, it seems unlikely it would be able to acquire enough power to do so; it would surely make similarly misjudged mistakes along the way, such as breaking social norms, that would lead to it being shut down.</p>
<p>Technical constraints, as advocated for by the AI safety community, are likely infeasible. A better way to mitigate AI harms is to focus on the risks of how people can misuse AI such as cybersecurity, biological misuse, and disinformation. Portraying AI as all-powerful overstates its capabilities and underemphasises its limitations, playing into the hands of companies that prefer less scrutiny and making it less likely that people will spot and challenge AI snake oil.</p>
<p>Rather than scaling AI progress in terms of ‚Äúintelligence‚Äù, which is hard to measure in any meaningful way, perhaps it is better to measure progress in terms of power. Power, defined as the ability to modify the environment, aptly frames how technological capability has accelerated at certain points throughout history. A focus on power reframes the argument to demonstrate that it is <em>us</em> who are being altered and made more powerful. Shifts in power dynamics will continue as AI capabilities improve: ‚Äúwe are the ‚Äòsuperintelligent‚Äô beings‚Äù and we should be far more concerned with what people do with AI than what AI will do on its own.</p>
</section>
<section id="id12">
<h2>Discussion Summary<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<section id="the-authors-defined-agi-as-ai-that-can-perform-most-or-all-economically-relevant-tasks-as-effectively-as-any-human-they-noted-this-was-a-pragmatic-and-less-philosophical-definition-what-do-you-think-of-this-choice-what-are-the-consequences-of-this-definition">
<h3>The authors defined AGI as ‚ÄúAI that can perform most or all economically relevant tasks as effectively as any human‚Äù. They noted this was a pragmatic and less philosophical definition. What do you think of this choice? What are the consequences of this definition?<a class="headerlink" href="#the-authors-defined-agi-as-ai-that-can-perform-most-or-all-economically-relevant-tasks-as-effectively-as-any-human-they-noted-this-was-a-pragmatic-and-less-philosophical-definition-what-do-you-think-of-this-choice-what-are-the-consequences-of-this-definition" title="Link to this heading">#</a></h3>
<p>The definition used in the chapter is pragmatic: a ‚Äúgood enough‚Äù definition that we can actually use. Coming up with a precise and detailed definition is challenging, given the dependence on philosophical contentious concepts like intelligence, sentience, and phenomenology. A focus on human misuse of AI is more approachable than trying to answer these big philosophical questions. Some of us felt reassured by the definition used in the chapter, feeling that a pragmatic approach presents a more accessible way of tackling the AGI question.</p>
<p>Others among us felt that the chapter‚Äôs definition of AGI is too vague, and there is not enough time spent discussing it. The chapter seems to simply present the definition and move on, implying an assumed binary view of rightness through absolutive argument techniques that rule out other views and skim over the fact that there is an absence of consensus in defining AGI. As a society, we haven‚Äôt coalesced around a central view; we do have different opinions about what AGI might look like, so it is worthwhile and important to consider the range of views. ‚ÄúRelevant‚Äù is too vague: we wanted to know who the tasks are relevant to (e.g. are artists economically relevant?); what threshold constitutes relevance (e.g. are tasks only relevant when they reach a certain volume of effect?); and how widespread relevant tasks are. We were unclear as to whether there is some minimum or maximum limit to economic relevancy, for example, a maximum economically relevant task could be the best way of distributing humanitarian aid.</p>
<p>Tying general intelligence to economic value is a non-trivial claim and presents some interesting questions. Economic value is not usually what we are talking about when we discuss intelligence; we do not measure the progress of children at school by how much money they are worth. Yet, the connection presents some practical advantages; economic value can be readily quantified, making it possible to apply metrics to intelligence. It also draws attention to the motivations of those behind the pursuit of AGI, as companies are working to develop technologies that can replace human workers and generate more economic value for them. We see these motivations playing out in the way that AI is marketed as a tool for improving efficiency in all types of businesses.</p>
<p>Companies, especially big tech companies, make a lot of money out of AI and the myths that surround it, influencing the kinds of definitions those companies promote in the media. We recently encountered an AI researcher who was surprised at a big claim made by Sam Altman; we were surprised at the idea that Altman would ever be saying or attempting to say true things. Altman‚Äôs public image, and those of figures like him, are tailored towards making profit and strengthening their commercial interests. This is conceptually different to the intentions of other expert figures. For example, in his book ‚Äú<a class="reference external" href="https://www.penguin.co.uk/books/455809/the-trading-game-by-stevenson-gary/9781802062731">The Trading Game</a>‚Äù author Gary Stevenson is not arguing for something that will make a difference to his financial position (outside of the sales of the book), strengthening the trustworthiness of his argument. Altman, on the other hand, stands to profit immensely from his claims regarding AGI.</p>
</section>
<section id="in-ai-safety-policy-entrenched-camps-have-developed-with-vastly-divergent-views-on-the-urgency-and-seriousness-of-catastrophic-risks-from-ai-while-research-and-debate-are-important-policymakers-must-make-decisions-in-the-absence-of-expert-consensus-how-should-they-go-about-this-taking-into-account-differences-in-beliefs-as-well-as-values-and-stakeholders-interests">
<h3>In AI safety policy, entrenched camps have developed, with vastly divergent views on the urgency and seriousness of catastrophic risks from AI. While research and debate are important, policymakers must make decisions in the absence of expert consensus. How should they go about this, taking into account differences in beliefs as well as values and stakeholders‚Äô interests?<a class="headerlink" href="#in-ai-safety-policy-entrenched-camps-have-developed-with-vastly-divergent-views-on-the-urgency-and-seriousness-of-catastrophic-risks-from-ai-while-research-and-debate-are-important-policymakers-must-make-decisions-in-the-absence-of-expert-consensus-how-should-they-go-about-this-taking-into-account-differences-in-beliefs-as-well-as-values-and-stakeholders-interests" title="Link to this heading">#</a></h3>
<p>Policymakers should leverage expert opinion to inform decision-making regarding AI, however, defining what constitutes expertise is <a class="reference external" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3925425/#:~:text=An%20expert%20is%20commonly%20defined,not%20possessed%20by%20most%20people">tricky and often unclear</a>. Expert opinion should be considered alongside the idea of <a class="reference external" href="https://dataethicsclub.com/write_ups/DataFeminism_writeup.html#data-feminism-chapter-6-the-numbers-dont-speak-for-themselves">situated knowledge</a>, which emphasises the importance of the context in which knowledge was produced. A balanced view of expertise should be considered, similar to how experts are used in court cases to present supporting evidence for explanations. In court, the judge and jury must compare each side and fathom which argument is most credible. The duty of expert witnesses is to the court, rather than to the defence or prosecution. Similarly, experts working with policymakers should view their duty as to society, rather than to the interests of a particular company or organisation. It can be more difficult to quantify conflicts of interest in the intellectual realm compared to economic conflicts of interest.</p>
<p>Whilst a balanced view of diverse expert opinion supports more informed decision-making, on a practical level there are some scenarios (such as the pandemic) where it is almost unhelpful for experts to cast doubt on scientific messaging. We wondered if disagreement among experts is also unhelpful in other policy situations. As scientists, we embrace discussion and disagreement, but scientific disagreement isn‚Äôt always appreciated by the press and politicians.</p>
<p>There is something about sitting in ambiguity that many people don‚Äôt like. Often, decision-makers would prefer to justify the course of action as being correct, rather than admitting that they aren‚Äôt certain what is best but have decided upon a particular route because of some justifiable reasons. In our experience as expert witnesses, we have found that we have to dress up the evidence in a way that is appealing and gives people a sense of security. Especially in medical settings, people want to be really clear about what‚Äôs happening to them. In saying this, research has also found that <a class="reference external" href="https://dataethicsclub.com/write_ups/2024-10-28_writeup.html">people are more comfortable with uncertainty than we might assume</a>.</p>
<p>To make decisions about AI policy, it is important that policymakers consider a wide scope of possible risks. Threats policymakers should be taking seriously include the impact of AI on education, the environment, and cognitive functioning; we would have liked more discussion about these kinds of risks in the chapter. AI can cause significant threat to society through its integration into vital infrastructure, such as banking systems. We have seen the harm that faulty AI can cause, such as the <a class="reference external" href="https://www.bbc.co.uk/news/articles/c1wpp4w14pqo">Robodebt</a> and <a class="reference external" href="https://www.bbc.co.uk/news/articles/c1wpp4w14pqo">Post Office</a> scandals. AI also poses risks to the integrity of creative industries and the propensity of creatives to generate an income; the <a class="reference external" href="https://fortune.com/europe/2025/05/13/british-government-under-fire-artists-dua-lipa-paul-mccartney-harry-potter-ai-copyright-exception/">UK government has proposed a copyright exemption for commercial generative AI training</a>.</p>
<p>We wondered if there would be more regulation and transparency if AI tools were primarily developed by non-profit companies. <a class="reference external" href="https://medium.com/&#64;DiscoverLevine/a-timeline-of-openais-technology-funding-and-history-c91cbc071a85">OpenAI was originally non-profit, but transitioned to a for-profit model in 2019</a>. We wondered if OpenAI was acting mainly in the interests of its funders when the company was non-profit, despite claiming that its aim was ‚Äú<a class="reference external" href="https://openai.com/index/introducing-openai/">to build value for everyone rather than shareholders</a>‚Äù ‚Äì shareholders that included <a class="reference external" href="https://techfundingnews.com/who-is-funding-openais-rise-to-the-top/">big tech companies such as Amazon and figures like Peter Theil</a>.</p>
</section>
<section id="how-much-do-you-think-the-risk-is-the-users-of-ai-versus-the-ai-itself-how-does-this-inform-how-we-should-address-specific-issues-related-to-ai-such-as-deepfakes-cybersecurity-or-the-ability-to-synthesise-new-bioweapons">
<h3>How much do you think the risk is the users of AI versus the AI itself? How does this inform how we should address specific issues related to AI such as deepfakes, cybersecurity or the ability to synthesise new bioweapons?<a class="headerlink" href="#how-much-do-you-think-the-risk-is-the-users-of-ai-versus-the-ai-itself-how-does-this-inform-how-we-should-address-specific-issues-related-to-ai-such-as-deepfakes-cybersecurity-or-the-ability-to-synthesise-new-bioweapons" title="Link to this heading">#</a></h3>
<p>Risks accompany the automation of work, as many companies attempt to replace human workers with AI systems, leaving many peoples vulnerable to losing their jobs. Companies may be concerned that if they do not replace humans with AI, they will fall behind companies that do. However, the job landscape has undergone significant shifts in the past, such as the industrial revolution or after WWII, with some jobs being lost, and others emerging. We also wondered at what level AI can actually replace a person. Given the way that AI has been implemented in administrative roles, it seems that AI doesn‚Äôt need to be as good as a person for that person to be replaceable. The question of AI adoption is thus whether it can do the job that people want, rather than the job you were actually doing. We‚Äôve seen evidence of this in jobs we‚Äôve had where people just want <em>an</em> answer, and don‚Äôt particularly care about if it is the correct or the best answer. Some managers just see that a task needs to be done, but do not worry about how it is done or how good the outcome is; a sentence needs to be written, but the details of what‚Äôs inside don‚Äôt matter. We‚Äôve experienced the pressure of high workloads forcing us into situations where we‚Äôve had to cut corners, and we can see how AI could be useful in these circumstances. Yet we also see people think not that they are cutting corners by using AI, but that AI is helping them do something quicker and better. These people fail to understand that the output produced by AI might actually be worse that what they would do themselves.</p>
<p>In general, AI is a way to do things that humans already do, but much faster and in higher volume. The speed and magnitude of AI outputs elevates the level of risk, and highlights the importance of counteracting problematic human tendencies such as using art without credit or placing too much trust in technology. AI is arguably becoming too accessible; it is now harder not to use AI that it is to use it, as it is automatically integrated into phone operating systems and forms the first output of queries in many search engines.</p>
<p>As well as the users of AI and AI itself, when considering AI risks we need to consider the salesmen: the snake oilers. With snake oil, the product itself was completely harmless; the problem lay mainly with the people trying to flog it and the gullibility of those who were buying it. With AI, the technology itself is not exactly harmless, but it does need to be discussed in relation to those who are trying to sell it. We are much less concerned about technology in isolation than we are about the promoters and users of it. Those who are pushing AI for profit should be held under careful scrutiny, as there is a misalignment between their motivations and the public good.</p>
<p>Intention is important, but people can believe that they are doing something good without seeing the bigger picture and thereby causing damage. AI does not have to be superintelligent to be risky; it is risky for humans to deploy AI without due diligence or setting up monitoring pipelines. Instagram algorithms, for example, are not particularly intelligent but still pose a lot of risks for teenagers.
The noise around existential AI risks seem to be underpinned by a depiction of AI as great and powerful. We must pay attention to the layers of the message being conveyed: we should be scared of AI because it is risky, but it is risky because it is powerful, more powerful than humans, lending force to the idea that humans should defer to AI. This is problematic: AI today is not all-powerful or all-knowing. Continuing to talk about AI as the next big thing without thinking critically about what the underlying message is or where it is coming from, people will manifest it into reality. People need to be aware of the autonomy and power they have in propagating this narrative.</p>
<p>Society must combat misinformation about AI capabilities so that people are able to interact with it appropriately. People seem to question AI less than other technologies (such as <a class="reference external" href="https://mydentalclinic.ca/blog/the-fluoride-toothpaste-conspiracy-dissecting-the-truth/">toothpaste</a>, for example). We wondered whether people have a responsibility to check the information that is being relayed to them before they pass it on to others. It would be impractical to do this for all information we encounter, as it would occupy too much time and resources. There seems to be some threshold of when we check something, for example the level (or perceived level) of expertise of who we are talking to. For the news, we know the bias or political leaning of particular news outlets, and this informs the lens that we read it through.</p>
<p>Misinformation can be countered by better education about what AI can or can‚Äôt do and how its inner mechanisms work. Responsibility for educating people about AI is diffuse. AI companies are primarily financially motivated, making it difficult to hold them to account for facilitating accurate education about AI. Governments have influence over the educational system, yet have demonstrated enthusiastic adoption of AI, especially <a class="reference external" href="https://www.ft.com/content/b02ba1bd-1075-4703-9b7d-800e2efa4513">in the UK</a> where the government has created <a class="reference external" href="https://www.gov.uk/government/news/prime-minister-sets-out-blueprint-to-turbocharge-ai">AI growth zones to fast track planning for AI infrastructure</a>. The favour of AI in the UK government makes us question if we can trust the government to drive accurate education about what is and isn‚Äôt feasible with AI. Politicians may not be the best people to design AI education, as they themselves may not be very educated about AI; in the US, the education secretary didn‚Äôt even know what it was called, <a class="reference external" href="https://eu.usatoday.com/story/news/politics/2025/04/12/linda-mcmahon-a1-instead-of-ai/83059797007/">referring to AI as ‚ÄúA1‚Äù</a>. As politicians hold occupy positions of power, perhaps it is their responsibility to educate themselves about AI. Issues with lobbying and corruption may influence politics, especially given the amount of money associated with AI.</p>
</section>
<section id="id13">
<h3>What change would you like to see on the basis of this piece? Who has the power to make that change?<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>We should hold AI products to the same standards as other (e.g. diagnostic) tools. There should be someone that can be held accountable for any decisions; someone who checks the outputs and makes the final call. Human accountability is crucial because we can‚Äôt just blame the tools; tools cannot feel remorse or make efforts to correct mistakes.</p>
<p>Whilst important, human control is not a tick box and must be careful thought out. It is a very human tendency to avoid blame, so efforts must be made to mitigate this. There have been instances of <a class="reference external" href="https://futurism.com/tesla-nhtsa-autopilot-report">Tesla cars giving control back to human drivers split seconds before a collision</a>, which is clearly not enough time for a person to course correct. Regardless of how effective this is, as the control has been returned to the human, there is room for Tesla to claim that the fault of the accident does not lie with the technology but with the human.</p>
<p>AI does make mistakes, but we think it will continue to improve as development progresses. Some of us self-describe as AI optimists and have high hopes for the potential of AI to benefit humanity, aside from the climate impact which we acknowledge needs addressing. Copyright issues could conceivably be worked out in the future, as the legal system adapts to the new landscape we find ourselves in. There will probably still be a demand for artists and creatives, as there is something special about the human element in these domains.</p>
<p>At the scale of AI use within organisations, we do think it will be possible to turn it off or stop using it. We wondered what the ethical route is if you realise your company is causing harm; whether you just stop working and let others take over, or if it is possible to switch paths and keep doing the work in a more ethical way. For individuals, we think that stopping usage of AI might be harder, depending on what their skills are and how comfortable they are with the technology.</p>
</section>
<section id="id14">
<h3>Attendees<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Huw Day, Data Scientist, University of Bristol: <a class="reference external" href="https://www.linkedin.com/in/huw-day/">LinkedIn</a>, <a class="reference external" href="https://bsky.app/profile/huwwday.bsky.social">BlueSky</a></p></li>
<li><p><a class="reference external" href="https://jessica-woodgate.github.io/">Jessica Woodgate</a>, PhD Student, University of Bristol</p></li>
<li><p>Liz Ing-Simmons, RSE, King‚Äôs College London :wave:</p></li>
<li><p>Virginia Scarlett, Open Source Programs Specialist, UC Santa Barbara :coffee:</p></li>
<li><p>Beverly Shirkey, Medical Statistician (Clinical Trials) University of Bristol :smile:</p></li>
<li><p>Paul Matthews, Lecturer, UWE Bristol</p></li>
<li><p>Jessica Bowden, Research Associate, University of Bristol :cat:</p></li>
<li><p>Robin Dasler, data product manager, California</p></li>
<li><p>Julie-Myrtille Bourgognon, lecturer, University of Glasgow</p></li>
</ul>
</section>
</section>
</section>

<div class="section ablog__blog_comments">
  
  


<div class="section ablog__prev-next">
  <span class="ablog__prev">
    
    
    <a href="2025-04-30_writeup.html">
      
      <i class="fa fa-arrow-circle-left"></i>
      
      <span>Data Ethics Club: UK announces AI funding for teachers: how this technology could change the profession</span>
    </a>
    
  </span>
  <span class="ablog__spacer">&nbsp;</span>
  <span class="ablog__next">
    
  </span>
</div>

  
  
</div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Data Ethics Club: AI Snake Oil Book Club</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-1-introduction">Chapter 1 ‚Äì Introduction</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-summary">Discussion Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-easy-and-hard-mean-in-the-context-of-ai-does-it-refer-to-computational-requirements-or-the-human-effort-needed-to-build-ai-to-perform-a-task-or-something-else-and-what-does-easy-hard-for-people-mean">What do ‚Äúeasy‚Äù and ‚Äúhard‚Äù mean in the context of AI? Does it refer to computational requirements, or the human effort needed to build AI to perform a task, or something else? And what does easy/hard for people mean?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#based-on-your-definitions-of-these-terms-pick-a-variety-of-tasks-and-try-to-place-them-on-a-2-dimensional-spectrum-where-the-axes-represent-peoples-and-computers-ease-of-performing-the-task-what-sort-of-relationship-do-you-see">Based on your definitions of these terms, pick a variety of tasks and try to place them on a 2-dimensional spectrum where the axes represent people‚Äôs and computers‚Äô ease of performing the task. What sort of relationship do you see?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-text-gives-many-examples-of-ai-that-quietly-work-well-like-spellcheck-can-you-think-of-other-examples-what-do-you-think-are-examples-of-tasks-that-ai-cant-yet-perform-reliably-but-one-day-will-without-raising-ethical-concerns-or-leading-to-societal-disruption">The text gives many examples of AI that quietly work well, like spellcheck. Can you think of other examples? What do you think are examples of tasks that AI can‚Äôt yet perform reliably but one day will, without raising ethical concerns or leading to societal disruption?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-change-would-you-like-to-see-on-the-basis-of-this-piece-who-has-the-power-to-make-that-change">What change would you like to see on the basis of this piece? Who has the power to make that change?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attendees">Attendees</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-2-how-predictive-ai-goes-wrong">Chapter 2 - How predictive AI goes wrong</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Discussion Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-models-make-common-sense-mistakes-that-people-would-catch-like-predicting-that-patients-with-asthma-have-a-lower-risk-of-developing-complications-from-pneumonia-as-discussed-in-the-chapter-what-if-anything-can-be-done-to-integrate-common-sense-error-checking-into-predictive-ai">Predictive models make ‚Äúcommon sense‚Äù mistakes that people would catch, like predicting that patients with asthma have a lower risk of developing complications from pneumonia, as discussed in the chapter. What, if anything, can be done to integrate common-sense error checking into predictive AI?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#think-about-a-few-ways-people-game-decision-making-systems-in-their-day-to-day-life-what-are-ways-in-which-it-is-possible-to-game-predictive-ai-systems-but-not-human-led-decision-making-systems-would-the-types-of-gaming-you-identify-work-with-automated-decision-making-systems-that-do-not-use-ai">Think about a few ways people ‚Äúgame‚Äù decision-making systems in their day-to-day life. What are ways in which it is possible to game predictive AI systems but not human-led decision making systems? Would the types of gaming you identify work with automated decision-making systems that do not use AI?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#in-which-kinds-of-jobs-are-automated-hiring-tools-predominantly-used-how-does-adoption-vary-by-sector-income-level-and-seniority-what-explains-these-differences">In which kinds of jobs are automated hiring tools predominantly used ? How does adoption vary by sector, income level, and seniority? What explains these differences?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">What change would you like to see on the basis of this piece? Who has the power to make that change?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Attendees</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-3-why-cant-ai-predict-the-future">Chapter 3 - Why can‚Äôt AI predict the future?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Discussion Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-authors-list-3-main-criteria-for-good-prediction-use-cases-real-world-utility-moral-legitimacy-and-irreducible-error-error-that-wont-go-away-with-more-data-and-better-methods-is-this-list-complete">The authors list 3 main criteria for good prediction use cases: real world utility, moral legitimacy, and irreducible error (error that won‚Äôt go away with more data and better methods). Is this list complete?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-advantage-implies-a-lot-of-success-comes-down-to-luck-which-challenges-a-meritocratic-world-view-how-do-you-feel-about-this">Cumulative advantage implies a lot of success comes down to luck, which challenges a meritocratic world view. How do you feel about this?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#social-sciences-focus-on-understanding-causal-mechanisms-not-predicting-associations-as-opposed-to-typical-machine-learners-what-do-both-fields-have-to-learn-from-one-another">Social sciences focus on understanding causal mechanisms, not predicting associations as opposed to typical machine learners. What do both fields have to learn from one another?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">What change would you like to see on the basis of this piece? Who has the power to make that change?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Attendees</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-4-the-long-road-to-generative-ai">Chapter 4 - The Long Road to Generative AI</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Discussion Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#various-ai-systems-such-as-the-neural-networks-in-the-imagenet-competitions-are-designed-to-automate-something-that-is-expensive-to-do-on-mass-e-g-labelling-images-and-where-the-process-in-doing-so-is-not-valuable-to-the-humans-doing-it-what-tasks-do-you-see-genai-both-llms-and-image-generators-being-used-for-and-how-much-are-they-being-used-for-processes-where-there-is-value-for-humans-performing-it">Various AI systems, such as the neural networks in the ImageNet competitions, are designed to automate something that is expensive to do on mass (e.g. labelling images) and where the process in doing so is not valuable to the humans doing it. What tasks do you see GenAI (both LLMs and image generators) being used for and how much are they being used for processes where there is value for humans performing it?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-ai-is-built-using-the-creative-output-of-journalists-writers-photographers-artists-and-others-generally-without-consent-credit-or-compensation-discuss-the-ethics-of-this-practice-how-can-those-who-want-to-change-the-system-go-about-doing-so-can-the-market-solve-the-problem-such-as-through-licensing-agreements-between-publishers-and-ai-companies-what-about-copyright-law-either-interpreting-existing-law-or-by-updating-it-what-other-policy-interventions-might-be-helpful">Generative AI is built using the creative output of journalists, writers, photographers, artists, and others ‚Äî generally without consent, credit, or compensation. Discuss the ethics of this practice. How can those who want to change the system go about doing so? Can the market solve the problem, such as through licensing agreements between publishers and AI companies? What about copyright law ‚Äî either interpreting existing law or by updating it? What other policy interventions might be helpful?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discuss-the-environmental-impact-of-generative-ai-what-if-anything-is-distinct-about-ais-environmental-impact-compared-to-computing-in-general-or-other-specific-digital-technologies-with-a-large-energy-use-such-as-cryptocurrency">Discuss the environmental impact of generative AI. What, if anything, is distinct about AI‚Äôs environmental impact compared to computing in general or other specific digital technologies with a large energy use such as cryptocurrency?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-5-is-advanced-ai-an-existential-threat">Chapter 5 - Is Advanced AI an Existential Threat?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Chapter Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Discussion Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-authors-defined-agi-as-ai-that-can-perform-most-or-all-economically-relevant-tasks-as-effectively-as-any-human-they-noted-this-was-a-pragmatic-and-less-philosophical-definition-what-do-you-think-of-this-choice-what-are-the-consequences-of-this-definition">The authors defined AGI as ‚ÄúAI that can perform most or all economically relevant tasks as effectively as any human‚Äù. They noted this was a pragmatic and less philosophical definition. What do you think of this choice? What are the consequences of this definition?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#in-ai-safety-policy-entrenched-camps-have-developed-with-vastly-divergent-views-on-the-urgency-and-seriousness-of-catastrophic-risks-from-ai-while-research-and-debate-are-important-policymakers-must-make-decisions-in-the-absence-of-expert-consensus-how-should-they-go-about-this-taking-into-account-differences-in-beliefs-as-well-as-values-and-stakeholders-interests">In AI safety policy, entrenched camps have developed, with vastly divergent views on the urgency and seriousness of catastrophic risks from AI. While research and debate are important, policymakers must make decisions in the absence of expert consensus. How should they go about this, taking into account differences in beliefs as well as values and stakeholders‚Äô interests?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-much-do-you-think-the-risk-is-the-users-of-ai-versus-the-ai-itself-how-does-this-inform-how-we-should-address-specific-issues-related-to-ai-such-as-deepfakes-cybersecurity-or-the-ability-to-synthesise-new-bioweapons">How much do you think the risk is the users of AI versus the AI itself? How does this inform how we should address specific issues related to AI such as deepfakes, cybersecurity or the ability to synthesise new bioweapons?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">What change would you like to see on the basis of this piece? Who has the power to make that change?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Attendees</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/very-good-science/data-ethics-club/edit/main//site/write_ups/AISnakeOil_writeup.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/write_ups/AISnakeOil_writeup.md.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      ¬© Copyright 2024, Data Ethics Club Community.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.4.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("G-93XN98JDFL");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>