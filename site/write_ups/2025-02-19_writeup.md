---
blogpost: true
date: February 19th, 2025
author: Jessica Woodgate
category: Write Up
tags: LLMs, generative AI, big tech, ML, open source
---
# Data Ethics Club: [OpenAI Furious DeepSeek Might Have Stolen All the Data OpenAI Stole From Us](https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/)

```{admonition} What's this? 
This is summary of Wednesday 19th February’s Data Ethics Club discussion, where we spoke and wrote about the New Republic article [OpenAI Furious DeepSeek Might Have Stolen All the Data OpenAI Stole From Us](https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/) by Jason Koebler.
The summary was written by Jessica Woodgate, who tried to synthesise everyone's contributions to this document and the discussion. "We" = "someone at Data Ethics Club". 
Huw Day helped with the final edit.
```

## Article Summary

In January 2025, [DeepSeek](https://www.deepseek.com/) – a Chinese [AI startup specialising in large language models (LLMs)](https://deepseek.net/about) – released its R1 model, which shocked the world by demonstrating better performance than [OpenAI](www.openai.com)’s models whilst, DeepSeek claim, costing significantly less money and using older chips. Unlike other LLMs, [DeepSeek’s approach develops reasoning capabilities using purely reinforcement learning (RL) techniques](https://medium.com/@mayadakhatib/deepseek-r1-a-short-summary-73b6b8ced9cf). 

Whilst the model was released [open source](https://github.com/deepseek-ai/DeepSeek-R1) and alongside [a (non-peer reviewed) research paper](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf), questions remain regarding the mechanics of R1’s training. [Model distillation](https://medium.com/stream-zero/understanding-the-essentials-of-model-distillation-in-ai-1e97403bee8a) is where smaller, faster “student” models are created by compressing knowledge of a larger, more complex “teacher” model. The student model is trained to replicate the teacher’s output distributions by asking the teacher lots of questions and mimicking the teacher’s reasoning process. Suspicious that R1 was trained using distillation, Microsoft and OpenAI are investigating whether DeepSeek used output obtained in an unauthorised manner from OpenAI.

The issue is raising questions about where the line is between using available resources to drive progress and stealing. In terms of using available resources, DeepSeek could be seen as following standard software development practice, wherein research generally works iteratively, taking into account what has been done before and building on that to create something novel. If DeepSeek’s methodology is cast as stealing, OpenAI could be seen as hypocritical. OpenAI itself has been widely criticised for scraping large amounts of data from the internet to train its systems, and is currently being sued by [the New York Times for unpermitted use of articles to train LLMs](https://harvardlawreview.org/blog/2024/04/nyt-v-openai-the-timess-about-face/).

## Discussion Summary

### Under what circumstances do you think training AI models using publicly available internet materials is fair use? How should copyright come into this? 

A side-effect of advances in AI is that copyright law is being put under the spotlight, as current legislation does not provide a straightforward answer regarding AI companies using public data. Currently, in the US [anyone may use a work that is in the public domain, but no-one can own it](https://copyrightalliance.org/faqs/freely-using-public-domain-materials/). [Derivative work is based on a work that has already been copyrighted, so that the new work derives from the previous work](https://www.legalzoom.com/articles/what-are-derivative-works-under-copyright-law). If you own copyright to a work, you also have right to derivative works, however, if the material is deemed sufficiently original and creative it is copyrightable by itself. 

There are [numerous undergoing lawsuits involving technology giants participating in AI development](https://sustainabletechpartner.com/topics/ai/generative-ai-lawsuit-timeline/) that claim the companies have trained their models on copyrighted materials without authorisation. Typically, defendants argue that they are protected by [fair use](https://en.wikipedia.org/wiki/Fair_use), a law that permits limited use of copyrighted material without obtaining permission. The intention of fair use is to protect creative works by permitting the use of copyrighted items to create something new, provided that the new artefact is sufficiently transformative. Some of the arguments in favour of generative AI as fair use rest on the premise that the outputs are not copies of the inputs and are thus similar to being inspired by something. From this premise, transformative use is argued as fair and not competing directly with what was inputted to create it. However, many creators and copyright owners disagree that the appropriation of their material to train models constitutes fair use.

To discern fair use of publicly available internet materials, it might help to clarify if and why there is a distinction between individuals reading and utilising publicly available data, and entities using publicly available data to train models. Yet, this distinction is not obvious to us. Using publicly available data to train models could be deemed as fair provided that users of publicly available data document what data is used, and how the data is used. Even if this definition is sufficient, we remained unsure whether the practices carried out by big tech align with fair use.

Disagreements surrounding fair use and generative AI are exemplified in the case of [Kadrey v. Meta](https://www.loeb.com/en/insights/publications/2023/12/richard-kadrey-v-meta-platforms-inc), wherein the plaintiffs claim direct copyright infringement based on derivative work theory. The plaintiffs allege that their books were used in training of [Meta’s LLaMA model](https://www.llama.com/): they argue that Meta trained LLaMA on [LibGen, a library of pirated books](https://libgen.mx/), and this decision was [greenlit by Zuckerberg](https://www.rollingstone.com/culture/culture-news/ai-meta-pirated-library-zuckerberg-1235235394/). Disclosed emails reveal that Meta staff discussed methods to filter text from LibGen to remove copyright indications, and some raised concerns about using [torrenting](https://www.howtogeek.com/816597/what-is-torrenting-and-why-do-people-warn-against-it/) to obtain data. It will be interesting to see what the effects of lawsuits that big tech and especially [OpenAI is involved in](https://analyticsindiamag.com/ai-trends/all-the-lawsuits-filed-against-openai/) will have on the next steps for DeepSeek.

Ambiguities in copyright law may be relevant to issues wider than AI, as the way that data is produced and handled is continually changing. On the consumer level, concerns about the handling of data have arisen in recent years from the ways that big tech companies collect and sell data to advertising agents. Beyond this, the suspected distillation of ChatGPT by DeepSeek represents how data collection is once more undergoing a rapid evolution, moving from scraping data on the internet to refining the product of that scraped data. Looking into the future, there are further implications for how data collection could be commercialised, such as conducting input analysis on the tasks being asked to ChatGPT, then mapping analysis to a customer base for fulfilling those tasks. 

Copyright will need to respond to the changing implications of data handling and knowledge generation; whilst it is not infallible, it is the system that we have and should be worked with to evolve with the problems it is aiming to address. For example, if web scraping is unfair, it is important to consider how copyright can be integrated into the web scraping space. In the UK, [the data (use and access) bill](https://bills.parliament.uk/bills/3825) is currently before parliament in the House of Commons. One of the suggestions in the bill is to improve transparency over [crawlers](https://www.howtogeek.com/731787/what-is-a-web-crawler-and-how-does-it-work/), which are programs that browse the web to find sites to add to search engines. If a website doesn’t want some or all of its pages to appear on a search engine, the crawl exclusion list is a file (called robots.txt) that dictates to crawlers which web pages to exclude. Enforcement of the bill would require operators of web crawlers and general-purpose AI models to disclose the identity of crawlers they use.

Amending and enforcing legislation has to be backed by the government in power, yet as governments and their agendas change, backing behind particular legislation may change. In the US, [copyright law has historically been advocated by government](https://en.wikipedia.org/wiki/History_of_copyright_law_of_the_United_States). However, [the current government’s deregulation goals for big tech](https://www.newsweek.com/donald-trump-ai-big-tech-regulations-deepseek-workplace-artificial-intelligence-2024397) may conflict with desires to update copyright law to protect consumer data.

Regulating copyright and fair use is important to protect the rights of data producers and the experience of people that use the internet. Knowledge that whatever we put online has the potential to be used and regurgitated in different forms via generative AI could heighten feelings of surveillance. It is not just the rights of those who produce the data that make copyright law important, but also the effects of the model once it has been trained on that data.

Outside of fair use and copyright law, we felt that people should not be confined to only feel anger when their copyright is infringed. Some of us thought that if you don’t own data or have explicit consent for using it or for web scraping, you should not use the data to train LLMs. Companies should respect clear opt outs, [as is expected in Europe. Europe even goes beyond respecting opt outs to requiring opt ins](https://europa.eu/youreurope/citizens/consumers/internet-telecoms/data-protection-online-privacy/index_en.htm). Data used to train LLMs has been taken from publicly available material which was not provided with consent to be reused in this manner.

### Should these companies be publicly owed as a way to nationalise or democratise AI, since the models are trained on publicly available data?

The question of whether a company should be publicly owned is a political question, not an ethical or technical question. On the one hand, if AI is really going to be as transformative as the hype bubble suggests, perhaps it should be publicly accountable instead of led by commercial interest. Currently, decision making in AI development is top-down where a few big players have control over driving innovation and setting benchmarks. When using LLMs, you are implicitly subscribing to the ideologies of the companies and power structures behind their development. Redirecting the driving force and guidance for AI development to come from the ground up may help society regain control over the direction of travel and ensure developments are for the benefit of all. Democratising generative AI could involve some sort of voting system. On the other hand, it is not clear how this would work in practice, given the logistically challenges and amount of money that is involved in the AI sector.

[Governments are becoming more explicitly involved in the development of AI](https://www.weforum.org/stories/2025/01/rewire-governments-ai-in-the-intelligent-age-meta/), and national support does make a difference in the AI sector. Empowering the general population to have influence over AI development will require improved [AI literacy](https://www.sciencedirect.com/science/article/pii/S2666920X21000357) so that we can have more informed conversations. The lack of literacy drives [hype cycles](https://dataethicsclub.com/write_ups/2024-12-18_writeup.html), which are societal dynamics that work by triggering emotions to subdue political and regulatory questions. However, empowering the public is different to nationalising a model to make it publicly owned. 

Another approach to harnessing the benefits of language models for society could be to train small language models on company documentation, rather than using large language models trained on web scraped data. Perhaps companies should be required to pay licensing fees in order to use publicly available data for training. Although, environmental concerns for the resources required to train models remain.

### What do you think best practices should be for model distillation (i.e. one model learning off another)? Is it really stealing if OpenAI trained their model using vast amounts of publicly available data collected through web scraping?

Best practices for training models should prioritise transparency, in order to facilitate public scrutiny and hold companies accountable. In contexts like medicine where the stakes are high, [transparency is key to maintain respect for patients’ autonomy](https://code-medical-ethics.ama-assn.org/ethics-opinions/transparency-health-care). One way to achieve transparency is by making models [open source](https://opensource.com/resources/what-open-source), which means making the design of models publicly accessible so that people can modify and share it. The programming language [python](https://www.python.org/), for example, is open source. The military uses open source software; the US [Department of Defence policy requires that commercial software comes with either a warranty or source code to that software can be maintained](https://dodcio.defense.gov/open-source-software-faq/). [Open source software has played an important role in intelligence in the Russia-Ukraine war](https://www.gov.uk/government/speeches/how-open-source-intelligence-has-shaped-the-russia-ukraine-war).

As well as transparency, open source brings [advantages for security by inviting scrutiny from a wider audience, allowing for vulnerabilities to be identified quickly](https://medium.com/tech-encyclopedia/what-is-open-source-software-benefits-and-community-impact-2f344aee4013). When models are open source, providing model cards further increases transparency by making it easier to see where copy-pasting or code reuse has occurred, for instance. However, the efficacy of model cards in practice is debatable as [there has been mixed evidence for how consistently they are filled out](https://arxiv.org/abs/2402.05160). When tools are open source there are also accountability challenges, as it is difficult to allocate ownership and trace work back to its original input, essentially anonymising the work.

We wondered whether it is appropriate for OpenAI to still have ‘open’ in their name, when it no longer centralises open source but [has shifted its structure](https://medium.com/@DiscoverLevine/48-hours-with-openai-insights-and-reflections-e4f27258b017) and values to prioritise the [pursuit of artificial general intelligence (AGI)](https://www.semafor.com/article/10/12/2023/openai-quietly-changed-its-core-values) and [profit](https://www.theverge.com/2024/12/27/24330131/openai-plan-transform-for-profit-company). OpenAI has an unusual business model, as it [transitioned from a non-profit organisation created by influential leaders in technology to a for-profit organisation in partnership with Microsoft. Revenue prominently comes from licensing agreements, subscription services, and partnerships (notably Microsoft with a $1 billion investment in 2019)](https://www.latterly.org/openai-business-model/). OpenAI (and DeepSeek, if distillation was used) have gained significant economic benefit from exploiting grey areas of property rights.

Contrasting the approaches of [other companies like OpenAI](https://www.itprotoday.com/ai-machine-learning/openai-is-not-open-source-but-neither-are-plenty-of-other-open-organizations), DeepSeek have chosen to [make R1 open source](https://github.com/deepseek-ai/DeepSeek-R1) in addition to [sharing technical details in a publicly available report](https://arxiv.org/abs/2501.12948). In sharing these details, DeepSeek demonstrate [more transparency than many other LLMs](https://spectrum.ieee.org/open-source-llm-not-open), however, there are still aspects of opacity such as [questions about what chips were used in training, how much it really cost to train](https://nationalinterest.org/blog/techland/deepseek-what-we-know-and-what-we-dont-know), and [what happens to users’ data](https://www.forbes.com/sites/torconstantino/2025/02/04/4-warnings-about-deepseek-you-need-to-know-before-using-it/). Systems have been criticised as claiming to be ‘open’ yet [remaining closed in important ways](https://www.nature.com/articles/s41586-024-08141-1). To investigate the missing pieces in understanding how R1 works, [HuggingFace are attempting to reverse engineer the R1 pipeline from DeepSeek’s tech report](https://github.com/huggingface/open-r1).

If DeepSeek did use distillation, we wondered why OpenAI have not used the same techniques to improve performance. Explanations could include that OpenAI has tried distillation, and it didn’t work as well or [model collapse](https://www.ibm.com/think/topics/model-collapse) happened. Model collapse is where training generative AI models on AI generated content leads to a decline in performance, because generative AI models produce datasets with less variation than the original data distributions. Alternatively, perhaps OpenAI are over committed to their existing architecture, in which case R1 could be positive catalyst for innovation by demonstrating ways to break out of existing dogma.

Regarding the attitudes of tech companies towards model distillation, rights of use, and property rights, it is interesting to investigate the appropriateness of OpenAI calling distillation stealing. Whilst there are ambiguities about how ethical it is to distil models without explicit consent or train models from data scraped off of the internet, it is not obvious whether this equates to stealing. On the one hand, if you are stealing from something that stole off something else, it is still stealing. On the other hand, [reusing other people’s code is common practice in dev culture](https://docs.github.com/en/get-started/learning-to-code/reusing-other-peoples-code-in-your-projects), where projects have chains of code reuse as [developers use code from other people who used it from other people themselves](https://www.reddit.com/r/ProgrammerHumor/comments/v3dzve/ctrlc_ctrlv/). The TV show [Silicon Valley](https://en.wikipedia.org/wiki/Silicon_Valley_(TV_series)) explores the effects of disruptive technologies and issues of property rights that accompany them. By labelling DeepSeek as stealing, [OpenAI has been criticised as hypocritical for protesting about practices that can be viewed as analogous to its own practices](https://futurism.com/openai-mockery-stole-work-deepseek), reflecting similarities between distillation and OpenAI scraping publicly available data.

![Cartoon of programming and plagiarism: ‘Middle school: “plagiarism is unacceptable; High school: “plagiarism is unacceptable”; University: “plagiarism is unacceptable”; Work: Programmers “Man, I stole your code.” “It’s not my code.”](https://preview.redd.it/ctrl-c-ctrl-v-v0-mkj6a0p8s8391.jpg?width=1080&crop=smart&auto=webp&s=4ca12291f7552d30e0eaa42dcb930639f5b57074)

There are interesting anthropological perspectives on the labelling of actions as stealing, for example, the influence of global politics: actors from western countries tend to be labelled as “innovators” whilst actors from other countries behaving in similar ways are labelled as “thieves”. The anthropologist [Cori Hayden](https://anthropology.berkeley.edu/cori-hayden) has written about the [enclosures of public knowledge](https://www.tandfonline.com/doi/full/10.1080/17530351003617602), and how the line between proper and improper copy is policed and influenced by global politics.

### DeepSeek was trained more cost effectively and with less powerful hardware but still performed as well as OpenAI’s model, attributed in part to its new architecture rather than just throwing more data + compute at the problem. Do you think that constrained environments can generally be a good catalyst for innovation?

Much of the push for AI innovation comes from the idea of the [“AI arms race”](https://www.forbes.com/sites/drewbernstein/2024/08/28/who-is-winning-the-ai-arms-race/) between primarily the US and China. We wondered how much we should care about this supposed race; whether it is [advancing technological development](https://opentools.ai/news/the-new-ai-arms-race-global-powers-vie-for-dominance), [risking the destruction of humanity](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) or just bluster orchestrated to [further cement the position of dominant players in the economy](https://platforms.substack.com/p/the-ai-arms-race-fallacy). It is unclear what countries involved in the arms race want to do with the AI they develop, whether they want to [use it for the benefit of citizens by creating economic value](https://www.forbes.com/sites/bernardmarr/2021/05/24/the-new-global-ai-arms-race-how-nations-must-compete-on-artificial-intelligence/), or [for the military and warfare](https://foreignpolicy.com/2023/04/11/ai-arms-race-artificial-intelligence-chatgpt-military-technology/).

Supporting China’s position in the supposed arms race, [Chinese media has focused on the technological breakthroughs achieved in R1](https://sundayguardianlive.com/opinion/what-chinese-media-and-experts-are-saying-on-ai-disruptor-deepseek), arguing that DeepSeek demonstrates China’s increasing capability to develop cutting-edge models independent to Western technology. However, there is conflicting evidence surrounding how R1 was trained. DeepSeek claims it did not use Nvidia H100 chips, which are banned in China under US export controls, but some Chinese reporting states that [DeepSeek did train R1 on Nvidia H100 chips](https://www.csis.org/analysis/deepseek-huawei-export-controls-and-future-us-china-ai-race).

Whilst there is debate around R1’s novelty, there are other examples of truly significant innovation that have come out of the AI sector. For example, the paper [“Attention Is All You Need”](https://dl.acm.org/doi/10.5555/3295222.3295349) introduced the [transformer architecture](https://www.geeksforgeeks.org/architecture-and-working-of-transformers-in-deep-learning/), a type of deep learning model that is highly effective in capturing dependencies and contextual relationships. Transformers use [attention mechanisms](https://www.geeksforgeeks.org/ml-attention-mechanism/) to focus on specific parts of the input sequence. Attention works especially well in natural language processing (NLP) tasks where the meaning of a sentence is generally influenced by its context. The paper provided the foundation for powerful large language models that harness generative pre-trained transformers (GPTs) to enable models to identify relationships between words and thereby retain relevant context. Transformers have been revolutionary in NLP domains such as translation, and [are also being used for time series forecasting](https://huggingface.co/blog/autoformer), which [predicts future trends based on historical data](https://www.geeksforgeeks.org/time-series-analysis-and-forecasting/).

Generally, however, we feel that “innovation” is largely used as a buzzword to inflate the capabilities of creators and importance of the tools they provide to move towards replacing human involvement with AI. We found it interesting when phrasing such as “knowledge” and making iterations “smarter” is used, when advances are essentially refinements of statistical probabilities. We define innovation as “developing novel methods to address new problems”. There is nothing inherently wrong with the pursuit of knowledge. Humans love a puzzle; finding creative solutions to problems is an essential part of the human experience where [“necessity is the mother of invention”](https://en.wikipedia.org/wiki/Necessity_is_the_mother_of_invention). The crux is how that knowledge is used.

Whilst there has been hype and overexaggeration of the capabilities of generative AI, it is important to acknowledge that the functionality of ChatGPT has evolved over time, moving from [conversation sequencing](https://www.cambridge.org/core/books/abs/cambridge-handbook-of-discourse-studies/sequence-organization-understanding-what-drives-talk/99C304161504D8C1B86595C1CB59E070), which utilises what a speaker has said to make a relevant response and ensure steps in a conversation are related, to [collaboration and provoking more actionable interactions](https://www.theverge.com/openai/619352/chatgpt-tasks-operator-productivity). ChatGPT has been [criticised for being difficult to verify as it does not provide citations, although it is getting better at referencing sources and there are ways to encourage it to cite](https://www.zdnet.com/article/how-to-make-chatgpt-provide-better-sources-and-citations/).

Innovation should be focused to prevent resources being spent on generating tools that contribute little value; constraints mean that we can direct the aims of innovation. Some restrictions on development are necessary to ensure that products are ethical and do not exploit people. Innovation for the sake of innovation and the [move fast and break things](https://techcrunch.com/2015/03/10/move-fast-and-break-things/?guccounter=1) paradigm has real effects on real peoples’ lives. Constraints shouldn’t be aimed at [preventing super intelligent AI](https://siliconangle.com/2025/03/08/ais-existential-risks-separating-hype-reality/), but should be focused on the actual harms that are happening today, such as misuse of data and environmental impact. To foster benevolent innovation, players should be supported in pursuing knowledge, and then letting society choose the constraints for the application of that innovation.

In the domain of language models, constraining the size of models could enhance human creativity, rather than restrict or bypass it. When we have access to large and adaptable LLMs, it can be tempting to try and use those tools for every problem. Yet, these tools frequently [hallucinate](https://medium.com/@gcentulani/understanding-hallucination-in-llms-causes-consequences-and-mitigation-strategies-b5e1d0268069) and have [substantial environmental impact in terms of resources consumed in training and data storage](https://www.techtarget.com/searchenterpriseai/tip/Assessing-the-environmental-impact-of-large-language-models).

Constraints could also improve the quality of content being generated. Initially, there was a presumption that [the more data, the better. However, the fact that it is possible to obtain more data does not entail that the result will be better. There are real costs to having too much data](https://www.forbes.com/councils/forbestechcouncil/2022/01/03/why-yes-there-is-such-a-thing-as-too-much-data-and-why-you-should-care/). LLMs are also contributing to the [enshittification of the internet](https://www.wired.com/story/tiktok-platforms-cory-doctorow/), as the stakeholders being valued shifts from the users to shareholders. Devaluing the experience of users leads to [the web slowly being filled with generated low quality slop](https://gizmodo.com/enshittification-is-officially-the-biggest-word-of-the-year-2000530173) by AI enhanced search engine optimisation (SEO) farming websites. More data entails more noise, masking any important and meaningful content. 

There are physical resource constraints that digital technologies are subject to, despite the conceptual distancing. Big tech’s [energy demands are set to steadily increase](https://www.geekwire.com/2025/as-ai-booms-heres-how-microsoft-and-amazon-are-coming-up-with-energy-solutions/), and there are competing interests between their demands and the demands of the state for energy generation. [Datacentres also use an huge amount of water to keep compute equipment cool](https://www.theregister.com/2025/01/04/how_datacenters_use_water/), leading to concerns over water scarcity and desertification, especially as [datacentres are being built in already water restricted areas such as Arizona](https://www.businessinsider.com/arizona-running-out-of-water-data-centers-blame-microsoft-google-2023-6). 

### What change would you like to see on the basis of this piece? Who has the power to make that change?

LLMs have potential to do harm or good on a substantial level. [As LLMs are trained on data generated by society, they tend to reflect the stereotypes and prejudices that exist in the data they were trained on and thereby society](https://medium.com/@aaribhaider2008/understanding-bias-in-large-language-models-llms-b1f84a8e30ed). If there is [more information about one group of people in the training data, the model will more accurately predict that group](https://dataethicsclub.com/write_ups/2024-08-31_writeup.html#data-feminism-chapter-4-what-gets-counted-counts). If a model favours one group above another, it should not be deployed as it will exacerbate existing and possibly create further inequalities in populations that are already disadvantaged. 

The models that are biased and have been deployed, however, have shed a spotlight on inequalities that exist in society. Making these inequalities visible could help us to address them. Humans are biased, but it can be difficult to prove. For example, visiting a clinician in hospital could lead you to suspect that biases are influencing their diagnosis, yet it is often difficult to be sure of this. With AI models, it is possible to quantitatively measure bias and pinpoint where it exists.

## Attendees

- Huw Day, Data Scientist, University of Bristol: [LinkedIn](https://www.linkedin.com/in/huw-day/), [BlueSky](https://bsky.app/profile/huwwday.bsky.social)
- [Jessica Woodgate](https://jessica-woodgate.github.io/), PhD Student, University of Bristol
- Euan Bennet, Lecturer, University of Glasgow, [BlueSky](https://bsky.app/profile/dreuanbennet.bsky.social)
- Christina Palantza, PhD student, University of Bristol
- Vanessa Hanschke, Lecturer, University College London
- Arun Isaac, postdoc, University College London
- Michelle Venetucci, PhD student, Yale University
- [Kamilla Wells](https://www.linkedin.com/in/kamilla-wells/), Citizen Developer, Australian Public Service, Brisbane
- Mirah Jing Zhang, PhD student, Bristol Uni.
- Adrianna Jezierska, PhD student, University of Bristol [[LinkedIn](https://www.linkedin.com/in/adriannajezierska/)]
